{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "81b856e6-2015-485b-9da0-b97a547f6407",
      "metadata": {
        "id": "81b856e6-2015-485b-9da0-b97a547f6407"
      },
      "source": [
        "# Import necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a8e7836-54ac-48e5-b219-0e2642671b0f",
      "metadata": {
        "id": "4a8e7836-54ac-48e5-b219-0e2642671b0f"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import gensim.downloader as api\n",
        "from gensim.test.utils import datapath\n",
        "from gensim import utils\n",
        "import gensim.models\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import pos_tag\n",
        "from nltk.tokenize import word_tokenize\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "from sklearn.svm import LinearSVC\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import sys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f432d0a2-0bb5-4e27-857a-47863e1d40d7",
      "metadata": {
        "id": "f432d0a2-0bb5-4e27-857a-47863e1d40d7",
        "outputId": "c9097dfc-b89d-4c52-cd69-17d080e4e293"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pandas version: 2.1.4\n",
            "Gensim version: 4.3.2\n",
            "NLTK version: 3.8.1\n",
            "NumPy version: 1.26.2\n",
            "Torch version: 2.1.2\n",
            "TorchVision version: 0.16.2\n",
            "Python version: 3.9.12 (main, Apr  5 2022, 01:52:34) \n",
            "[Clang 12.0.0 ]\n"
          ]
        }
      ],
      "source": [
        "print(\"Pandas version:\", pd.__version__)\n",
        "print(\"Gensim version:\", gensim.__version__)\n",
        "print(\"NLTK version:\", nltk.__version__)\n",
        "print(\"NumPy version:\", np.__version__)\n",
        "print(\"Torch version:\", torch.__version__)\n",
        "print(\"TorchVision version:\", torchvision.__version__)\n",
        "print(\"Python version:\", sys.version)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44136efb-5ada-441d-bfeb-83ac63f6a746",
      "metadata": {
        "id": "44136efb-5ada-441d-bfeb-83ac63f6a746",
        "outputId": "946a6036-1336-4f89-bfa6-2ea100a89f20"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de910e0d-9cf2-46c2-8f13-4294373016b4",
      "metadata": {
        "id": "de910e0d-9cf2-46c2-8f13-4294373016b4",
        "outputId": "8858e7b1-d8ed-475b-9d44-471ad3eb8188"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     /Users/priyamvora/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     /Users/priyamvora/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /Users/priyamvora/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2fb76f2-8aab-4be0-b0a6-2450982c42ed",
      "metadata": {
        "id": "a2fb76f2-8aab-4be0-b0a6-2450982c42ed"
      },
      "source": [
        "# 1. Dataset Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1cc4755c-3270-42de-902c-b29b57ac0bf1",
      "metadata": {
        "id": "1cc4755c-3270-42de-902c-b29b57ac0bf1",
        "outputId": "fe985774-d7c3-4183-fecf-bfb1e6fcbbfd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/yv/3j35yf711vq84s93xhmkv6tr0000gn/T/ipykernel_5447/4101873849.py:1: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv('data.tsv',on_bad_lines='skip', sep='\\t')\n"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv('data.tsv',on_bad_lines='skip', sep='\\t')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "854521b9-44c4-4624-acad-f02f55cfad85",
      "metadata": {
        "id": "854521b9-44c4-4624-acad-f02f55cfad85"
      },
      "source": [
        "# Keep review and ratings and assign classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0163ad0-2a9a-4984-8267-b5fc5e02dc74",
      "metadata": {
        "id": "f0163ad0-2a9a-4984-8267-b5fc5e02dc74",
        "outputId": "f4aa229d-d596-4f56-e158-99491c01fa17"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of Reviews with Rating > 3: 2001052\n",
            "Number of Reviews with Rating <= 2: 445348\n",
            "Number of Reviews with Rating = 3: 193680\n"
          ]
        }
      ],
      "source": [
        "# keep only reviews and ratings\n",
        "df = df[['star_rating', 'review_body']]\n",
        "\n",
        "# Check for null values in the df\n",
        "df.isnull().any(axis=1).sum()\n",
        "df = df.dropna()\n",
        "\n",
        "# it seems that some values of star_rating are string while some are numeric. the below code will give an error and hence i was able to deduce this\n",
        "# df['sentiment'] = df['star_rating'].map(lambda x: 1 if x > 3 else 0 if x <= 2 else None)\n",
        "# df.shape\n",
        "\n",
        "# Convert 'star_rating' to numeric\n",
        "df['star_rating'] = pd.to_numeric(df['star_rating'], errors='coerce')\n",
        "df['star_rating'] = df['star_rating'].astype(int)\n",
        "\n",
        "# Get counts of reviews for each sentiment class\n",
        "reviews_greater_than_3 = df[df['star_rating'] > 3].shape[0]\n",
        "reviews_less_than_equal_2 = df[df['star_rating'] <= 2].shape[0]\n",
        "reviews_equal_3 = df[df['star_rating'] == 3].shape[0]\n",
        "\n",
        "print(\"Number of Reviews with Rating > 3:\", reviews_greater_than_3)\n",
        "print(\"Number of Reviews with Rating <= 2:\", reviews_less_than_equal_2)\n",
        "print(\"Number of Reviews with Rating = 3:\", reviews_equal_3)\n",
        "\n",
        "# create sentiment column\n",
        "df['sentiment'] = df['star_rating'].map(lambda x: 0 if x > 3 else 1 if x <= 2 else 2 if x == 3 else None)\n",
        "\n",
        "\n",
        "# convert sentiment to int type\n",
        "df['sentiment'] = df['sentiment'].astype(int)\n",
        "\n",
        "rating_one = df[df['star_rating'] == 1].sample(n=50000, random_state=42)\n",
        "rating_two = df[df['star_rating'] == 2].sample(n=50000, random_state=42)\n",
        "rating_three = df[df['star_rating'] == 3].sample(n=50000, random_state=42)\n",
        "rating_four = df[df['star_rating'] == 4].sample(n=50000, random_state=42)\n",
        "rating_five = df[df['star_rating'] == 5].sample(n=50000, random_state=42)\n",
        "\n",
        "downsized_df = pd.concat([rating_one, rating_two, rating_three, rating_four, rating_five])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bea80cc5-7ac8-4ec9-9e1e-19b9ac9fd382",
      "metadata": {
        "id": "bea80cc5-7ac8-4ec9-9e1e-19b9ac9fd382"
      },
      "source": [
        "Got 250000 ratings of each type of rating with classed assigned"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc838316-b705-4c64-862f-b32ec39c1e22",
      "metadata": {
        "id": "cc838316-b705-4c64-862f-b32ec39c1e22"
      },
      "source": [
        "# Preprocess and clean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f18569d-efab-4348-b1a9-b07eb42e7904",
      "metadata": {
        "id": "7f18569d-efab-4348-b1a9-b07eb42e7904"
      },
      "outputs": [],
      "source": [
        "contractions = {\"ain't\": 'am not / is not / are not / has not / have not', \"aren't\": 'are not', \"can't\": 'cannot', \"can't've\": 'cannot have', \"'cause\": 'because', \"could've\": 'could have', \"couldn't\": 'could not', \"couldn't've\": 'could not have', \"didn't\": 'did not', \"doesn't\": 'does not', \"don't\": 'do not', \"hadn't\": 'had not', \"hadn't've\": 'had not have', \"hasn't\": 'has not', \"haven't\": 'have not', \"he'd\": 'he would / he had', \"he'd've\": 'he would have', \"he'll\": 'he will', \"he'll've\": 'he will have', \"he's\": 'he is / he has', \"how'd\": 'how did', \"how'd'y\": 'how do you', \"how'll\": 'how will', \"how's\": 'how is', \"I'd\": 'I would / I had', \"I'd've\": 'I would have', \"I'll\": 'I will', \"I'll've\": 'I will have', \"I'm\": 'I am', \"I've\": 'I have', \"isn't\": 'is not', \"it'd\": 'it would / it had', \"it'd've\": 'it would have', \"it'll\": 'it will', \"it'll've\": 'it will have', \"it's\": 'it is / it has', \"let's\": 'let us', \"ma'am\": 'madam', \"mayn't\": 'may not', \"might've\": 'might have', \"mightn't\": 'might not', \"mightn't've\": 'might not have', \"must've\": 'must have', \"mustn't\": 'must not', \"mustn't've\": 'must not have', \"needn't\": 'need not', \"needn't've\": 'need not have', \"o'clock\": 'of the clock', \"oughtn't\": 'ought not', \"oughtn't've\": 'ought not have', \"shan't\": 'shall not', \"sha'n't\": 'shall not', \"shan't've\": 'shall not have', \"she'd\": 'she would / she had', \"she'd've\": 'she would have', \"she'll\": 'she will', \"she'll've\": 'she will have', \"she's\": 'she is / she has', \"should've\": 'should have', \"shouldn't\": 'should not', \"shouldn't've\": 'should not have', \"so've\": 'so have', \"so's\": 'so is', \"that'd\": 'that would', \"that'd've\": 'that would have', \"that's\": 'that is / that has', \"there'd\": 'there had', \"there'd've\": 'there would have', \"there's\": 'there is / there has', \"they'd\": 'they would / they had', \"they'd've\": 'they would have', \"they'll\": 'they will', \"they'll've\": 'they will have', \"they're\": 'they are', \"they've\": 'they have', \"to've\": 'to have', \"wasn't\": 'was not', \"we'd\": 'we would / we had', \"we'd've\": 'we would have', \"we'll\": 'we will', \"we'll've\": 'we will have', \"we're\": 'we are', \"we've\": 'we have', \"weren't\": 'were not', \"what'll\": 'what will', \"what'll've\": 'what will have', \"what're\": 'what are', \"what's\": 'what is / what has', \"what've\": 'what have', \"when's\": 'when is', \"when've\": 'when have', \"where'd\": 'where did', \"where's\": 'where is / where has', \"where've\": 'where have', \"who'll\": 'who will', \"who'll've\": 'who will have', \"who's\": 'who is / who has', \"who've\": 'who have', \"why's\": 'why is', \"why've\": 'why have', \"will've\": 'will have', \"won't\": 'will not', \"won't've\": 'will not have', \"would've\": 'would have', \"wouldn't\": 'would not', \"wouldn't've\": 'would not have', \"y'all\": 'you all', \"y'alls\": 'you alls', \"y'all'd\": 'you all would', \"y'all'd've\": 'you all would have', \"y'all're\": 'you all are', \"y'all've\": 'you all have', \"you'd\": 'you would / you had', \"you'd've\": 'you would have', \"you'll\": 'you you will', \"you'll've\": 'you you will have', \"you're\": 'you are', \"you've\": 'you have', \"who'd\": 'who would / who had', \"who're\": 'who are'}\n",
        "\n",
        "def expand_contractions(text):\n",
        "     for contraction, expansion_options in contractions.items():\n",
        "        # Select the first option when there are multiple choices\n",
        "        first_option = expansion_options.split('/')[0].strip()\n",
        "        text = text.replace(contraction, first_option)\n",
        "     return text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7763e38-f2c4-4e6f-ba80-b7f3734156de",
      "metadata": {
        "id": "a7763e38-f2c4-4e6f-ba80-b7f3734156de",
        "outputId": "039a627e-c8a5-4454-a4ee-906a72317536"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/yv/3j35yf711vq84s93xhmkv6tr0000gn/T/ipykernel_5447/803437059.py:2: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
            "  downsized_df['review_body'] = downsized_df['review_body'].apply(lambda x: ' '.join(BeautifulSoup(x, \"html.parser\").stripped_strings))\n",
            "/var/folders/yv/3j35yf711vq84s93xhmkv6tr0000gn/T/ipykernel_5447/803437059.py:2: MarkupResemblesLocatorWarning: The input looks more like a URL than markup. You may want to use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  downsized_df['review_body'] = downsized_df['review_body'].apply(lambda x: ' '.join(BeautifulSoup(x, \"html.parser\").stripped_strings))\n"
          ]
        }
      ],
      "source": [
        "downsized_df['review_body'] = downsized_df['review_body'].str.lower()\n",
        "downsized_df['review_body'] = downsized_df['review_body'].apply(lambda x: ' '.join(BeautifulSoup(x, \"html.parser\").stripped_strings))\n",
        "downsized_df['review_body'] = downsized_df['review_body'].str.replace('http[s]?://\\S+', '', regex=True)\n",
        "downsized_df['review_body'] = downsized_df['review_body'].str.replace(r'[^a-zA-Z ]', '', regex=True)\n",
        "downsized_df['review_body'] = downsized_df['review_body'].str.replace(' +', ' ', regex=True)\n",
        "downsized_df['review_body'] = downsized_df['review_body'].apply(expand_contractions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad8b35e4-c23e-4bae-8d64-eea1508e40b7",
      "metadata": {
        "id": "ad8b35e4-c23e-4bae-8d64-eea1508e40b7"
      },
      "outputs": [],
      "source": [
        "# lemmatizer = WordNetLemmatizer()\n",
        "# def process_stop_filtered_reviews(review):\n",
        "#     tokens = word_tokenize(review)\n",
        "#     pos_tags = pos_tag(tokens)\n",
        "#     pos_tags_mapped = [(word, get_wordnet_pos(tag)) for word, tag in pos_tags]\n",
        "#     lemmatized_tokens = [lemmatizer.lemmatize(word, pos=tag) if tag is not None else word for word, tag in pos_tags_mapped]\n",
        "#     return ' '.join(lemmatized_tokens)\n",
        "\n",
        "# def get_wordnet_pos(tag):\n",
        "#     if tag.startswith('N'):\n",
        "#         return 'n'\n",
        "#     elif tag.startswith('V'):\n",
        "#         return 'v'\n",
        "#     elif tag.startswith('R'):\n",
        "#         return 'r'\n",
        "#     elif tag.startswith('J'):\n",
        "#         return 'a'\n",
        "#     else:\n",
        "#         return None\n",
        "# downsized_df['review_body'] = downsized_df['review_body'].apply(process_stop_filtered_reviews)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "681489f0-fa5d-4137-ab04-578e8e260cef",
      "metadata": {
        "id": "681489f0-fa5d-4137-ab04-578e8e260cef"
      },
      "source": [
        "# Word Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1edb35e9-145a-4f78-9f05-be3b5c0f220a",
      "metadata": {
        "id": "1edb35e9-145a-4f78-9f05-be3b5c0f220a"
      },
      "outputs": [],
      "source": [
        "# using example link as reference - https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html\n",
        "wv = api.load('word2vec-google-news-300')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83d3837b-6737-4299-840c-4cb585de443f",
      "metadata": {
        "id": "83d3837b-6737-4299-840c-4cb585de443f",
        "outputId": "48696bfc-4e96-4f51-bcf3-80e609b6bc45"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.70098954\n"
          ]
        }
      ],
      "source": [
        "print(wv.similarity('cheap', 'inexpensive'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a2611ea-c7a0-4adf-9c3c-e5bc5c7e4059",
      "metadata": {
        "id": "8a2611ea-c7a0-4adf-9c3c-e5bc5c7e4059",
        "outputId": "0d5af073-1e68-4f6f-94ee-2b7399a9dace"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.08879091\n"
          ]
        }
      ],
      "source": [
        "print(wv.similarity('customer', 'helpful'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7c00ea2-7a9a-4d04-9f61-41ddd3783249",
      "metadata": {
        "id": "e7c00ea2-7a9a-4d04-9f61-41ddd3783249",
        "outputId": "16a04868-f29d-45e3-a679-b31738aa863a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.2354098\n"
          ]
        }
      ],
      "source": [
        "print(wv.similarity('exceed', 'expectation'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "618048ab-b770-4cac-b62e-a63b0836c23e",
      "metadata": {
        "id": "618048ab-b770-4cac-b62e-a63b0836c23e",
        "outputId": "146c0752-e58a-4f2e-d81d-29b6a9c190b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.60132354\n"
          ]
        }
      ],
      "source": [
        "print(wv.similarity('hate', 'dislike'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4dcc0f53-0707-4056-928f-1125a19c26d6",
      "metadata": {
        "id": "4dcc0f53-0707-4056-928f-1125a19c26d6",
        "outputId": "c5a4a9f9-868c-4d44-886a-01e732e2d69b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.25702554\n"
          ]
        }
      ],
      "source": [
        "print(wv.similarity('product', 'item'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ac71a8f-257d-47ef-9f73-18920a89255d",
      "metadata": {
        "id": "3ac71a8f-257d-47ef-9f73-18920a89255d",
        "outputId": "dff103a6-c53b-4ff3-b339-fb26267b5636"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.2341723\n"
          ]
        }
      ],
      "source": [
        "print(wv.similarity('knife', 'sharp'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ecfb251e-41b4-4609-b72b-efd3e183bdd9",
      "metadata": {
        "id": "ecfb251e-41b4-4609-b72b-efd3e183bdd9",
        "outputId": "244847c3-de85-4af7-e4c7-28312b409cb7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('queen', 0.7118192911148071), ('monarch', 0.6189674735069275), ('princess', 0.5902431011199951), ('crown_prince', 0.5499460697174072), ('prince', 0.5377321243286133), ('kings', 0.5236844420433044), ('Queen_Consort', 0.5235945582389832), ('queens', 0.5181134343147278), ('sultan', 0.5098593235015869), ('monarchy', 0.5087411403656006)]\n"
          ]
        }
      ],
      "source": [
        "print(wv.most_similar(positive=['woman', 'king'], negative=['man']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22f119e3-080c-4aa9-bfa1-de2598f56bd1",
      "metadata": {
        "id": "22f119e3-080c-4aa9-bfa1-de2598f56bd1",
        "outputId": "2dccb829-d300-4499-ab00-cfc2b9e0ece4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('kitchen_knife', 0.8097632527351379)]\n"
          ]
        }
      ],
      "source": [
        "print(wv.most_similar('knife', topn=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee9bd10e-cffc-4620-b3b6-3c372b46c826",
      "metadata": {
        "id": "ee9bd10e-cffc-4620-b3b6-3c372b46c826",
        "outputId": "78fc4e37-d37d-457e-ae15-9e8384f6a76b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('son', 0.8490633368492126),\n",
              " ('nephew', 0.7544960975646973),\n",
              " ('father', 0.7490662336349487),\n",
              " ('brother', 0.7456980347633362),\n",
              " ('grandson', 0.719298243522644),\n",
              " ('younger_brother', 0.7111448049545288),\n",
              " ('uncle', 0.6908944249153137),\n",
              " ('dad', 0.6855338215827942),\n",
              " ('sons', 0.6790387630462646),\n",
              " ('stepson', 0.6781994700431824)]"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wv.most_similar(positive=['daughter', 'man'], negative=['woman'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3f6bd1f-5efe-4482-89ab-7117840b6c4b",
      "metadata": {
        "id": "b3f6bd1f-5efe-4482-89ab-7117840b6c4b"
      },
      "outputs": [],
      "source": [
        "class ReviewsCorpus:\n",
        "    \"\"\"An iterator that yields sentences (lists of str).\"\"\"\n",
        "\n",
        "    def __init__(self, df):\n",
        "        self.df = df\n",
        "\n",
        "    def __iter__(self):\n",
        "        for review_body in self.df['review_body']:\n",
        "                yield utils.simple_preprocess(review_body)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "112400b5-d058-4e18-8fe2-078bf787d223",
      "metadata": {
        "id": "112400b5-d058-4e18-8fe2-078bf787d223"
      },
      "outputs": [],
      "source": [
        "corpus = ReviewsCorpus(downsized_df)\n",
        "wv_custom = gensim.models.Word2Vec(sentences=corpus, vector_size=300, window=11, min_count=10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "962abf62-6a9f-4e1d-95b9-38e35b45dda9",
      "metadata": {
        "id": "962abf62-6a9f-4e1d-95b9-38e35b45dda9",
        "outputId": "64d0f64f-ed19-479d-bdd5-51d4d2bd9610"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.5449395\n"
          ]
        }
      ],
      "source": [
        "print(wv_custom.wv.similarity('cheap', 'inexpensive'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c20399a-10f7-463e-8888-f3a99d2c8682",
      "metadata": {
        "id": "0c20399a-10f7-463e-8888-f3a99d2c8682",
        "outputId": "bf8ce94d-bd51-4911-9b8c-b712eae16395"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.2393541\n"
          ]
        }
      ],
      "source": [
        "print(wv_custom.wv.similarity('customer', 'helpful'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7574cfe-3c38-4779-b586-03a1e9806a9a",
      "metadata": {
        "id": "a7574cfe-3c38-4779-b586-03a1e9806a9a",
        "outputId": "f6bc997f-5156-4e46-ccbf-295efb618d8f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.38266268\n"
          ]
        }
      ],
      "source": [
        "print(wv_custom.wv.similarity('exceed', 'expectation'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8aa0ed1-49bf-4988-bf18-f2b55a5b3547",
      "metadata": {
        "id": "b8aa0ed1-49bf-4988-bf18-f2b55a5b3547",
        "outputId": "72480037-a1eb-4525-9474-5a9b54bf4509"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.4512464\n"
          ]
        }
      ],
      "source": [
        "print(wv_custom.wv.similarity('hate', 'dislike'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "579938df-92e3-45b0-9f7a-e53a6577534b",
      "metadata": {
        "id": "579938df-92e3-45b0-9f7a-e53a6577534b",
        "outputId": "bcdce5c5-6a5d-49fe-c1f9-7a5573404a10"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.76116943\n"
          ]
        }
      ],
      "source": [
        "print(wv_custom.wv.similarity('product', 'item'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f33f5085-ac4b-4159-8e0c-d1ec565935d7",
      "metadata": {
        "id": "f33f5085-ac4b-4159-8e0c-d1ec565935d7",
        "outputId": "347ba938-9aaf-47e7-e8f3-877ee6cbf3d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.39065558\n"
          ]
        }
      ],
      "source": [
        "print(wv_custom.wv.similarity('knife', 'sharp'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d44ef92-a62b-4d9b-a7d6-304bb7a49e10",
      "metadata": {
        "id": "3d44ef92-a62b-4d9b-a7d6-304bb7a49e10",
        "outputId": "f8946fb2-f194-437c-9073-a0ebf5a6dbb6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('british', 0.4386180639266968), ('noble', 0.4322429895401001), ('david', 0.4279828369617462), ('barnes', 0.42732563614845276), ('mead', 0.41298791766166687), ('kings', 0.4086189270019531), ('architecture', 0.4056771397590637), ('indiana', 0.4024195969104767), ('franklin', 0.3976956605911255), ('doll', 0.39660584926605225)]\n"
          ]
        }
      ],
      "source": [
        "print(wv_custom.wv.most_similar(positive=['woman', 'king'], negative=['man']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f687485e-9861-4beb-b46e-3e2a0412bbb7",
      "metadata": {
        "id": "f687485e-9861-4beb-b46e-3e2a0412bbb7",
        "outputId": "a08c3a21-5ab6-4d93-f23b-734f7bcd2533"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('blade', 0.7703830003738403)]\n"
          ]
        }
      ],
      "source": [
        "print(wv_custom.wv.most_similar('knife', topn=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97c60456-5ce0-485e-ac0d-9bde42056dfd",
      "metadata": {
        "id": "97c60456-5ce0-485e-ac0d-9bde42056dfd",
        "outputId": "86f115a2-f0dc-4ec6-c411-80bafecf5121"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('son', 0.7203930020332336), ('niece', 0.6802166104316711), ('grandson', 0.6796866655349731), ('granddaughter', 0.6793904900550842), ('husband', 0.6417809128761292), ('girlfriend', 0.6299337148666382), ('sister', 0.6271196007728577), ('dad', 0.61836177110672), ('daughters', 0.6152809858322144), ('sons', 0.6127235293388367)]\n"
          ]
        }
      ],
      "source": [
        "print(wv_custom.wv.most_similar(positive=['daughter', 'man'], negative=['woman']))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d75dc64-e381-4708-8d92-32110837cd92",
      "metadata": {
        "id": "3d75dc64-e381-4708-8d92-32110837cd92"
      },
      "source": [
        "# Conclusion : Custom Word2Vec vs PreTrained Word2Vec\n",
        "## Note - I have tried more than 2 pair of words to gain better understanding\n",
        "For some word pairs, the Google News model appears to capture semantic similarities better, while for others, the custom Amazon model shows higher similarity scores.\n",
        "For generic pairs of words which can appear in any context and not just Amazon reviews such as (cheap, inexpensive) or (hate, dislike), the Google News model performs significantly better.\n",
        "For words that are more likely to appear in a review, our custom model performs significantly better than pre trained model such as ('knife', 'sharp') or ('product', 'item')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6089457-c340-49aa-80e8-dc60f4b02236",
      "metadata": {
        "id": "a6089457-c340-49aa-80e8-dc60f4b02236"
      },
      "source": [
        "# Simple Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5dde4280-7c60-4fcd-a498-91339603bfbf",
      "metadata": {
        "id": "5dde4280-7c60-4fcd-a498-91339603bfbf"
      },
      "outputs": [],
      "source": [
        "# take positive and neg sentiments\n",
        "simple_df = downsized_df[downsized_df['sentiment'].isin([0, 1])]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6597327-8ccd-4a22-a23c-2d8138476c7c",
      "metadata": {
        "id": "a6597327-8ccd-4a22-a23c-2d8138476c7c"
      },
      "outputs": [],
      "source": [
        "def create_X_avg(df, word2vec_model):\n",
        "    X_avg = []\n",
        "\n",
        "    for i in range(df.shape[0]):\n",
        "        curr_review = df.iloc[i]['review_body']\n",
        "        curr_review = curr_review.replace(',', '')\n",
        "        curr_review = curr_review.replace('.', '')\n",
        "        curr_review = curr_review.split()\n",
        "        curr_vect = []\n",
        "\n",
        "        for word in curr_review:\n",
        "            if word in word2vec_model:\n",
        "                curr_vect.append(word2vec_model[word])\n",
        "\n",
        "        if len(curr_vect) == 0:\n",
        "            curr_vect = np.zeros(300, dtype=float)\n",
        "        else:\n",
        "            curr_vect = np.mean(curr_vect, axis=0)\n",
        "\n",
        "        X_avg.append(curr_vect)\n",
        "\n",
        "    return np.array(X_avg)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4dabea2-91c7-42b5-b368-e10e53207d01",
      "metadata": {
        "id": "c4dabea2-91c7-42b5-b368-e10e53207d01"
      },
      "outputs": [],
      "source": [
        "X_avg_pretrained = create_X_avg(simple_df, wv)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f2fa518-e5cc-4979-b155-79cdff48eb4e",
      "metadata": {
        "id": "9f2fa518-e5cc-4979-b155-79cdff48eb4e",
        "outputId": "7154879b-d46c-473a-e667-6021b670ff5e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(200000, 300)"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_avg_pretrained.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35b1ac20-3010-464e-9f25-f52f4747627e",
      "metadata": {
        "id": "35b1ac20-3010-464e-9f25-f52f4747627e"
      },
      "outputs": [],
      "source": [
        "def create_X_avg_custom(df, word2vec_model):\n",
        "    X_avg = []\n",
        "\n",
        "    for i in range(df.shape[0]):\n",
        "        curr_review = df.iloc[i]['review_body']\n",
        "        curr_review = curr_review.replace(',', '')\n",
        "        curr_review = curr_review.replace('.', '')\n",
        "        curr_review = curr_review.split()\n",
        "        curr_vect = []\n",
        "\n",
        "        for word in curr_review:\n",
        "            if word in word2vec_model.wv:\n",
        "                curr_vect.append(word2vec_model.wv[word])\n",
        "\n",
        "        if len(curr_vect) == 0:\n",
        "            curr_vect = np.zeros(300, dtype=float)\n",
        "        else:\n",
        "            curr_vect = np.mean(curr_vect, axis=0)\n",
        "\n",
        "        X_avg.append(curr_vect)\n",
        "\n",
        "    return np.array(X_avg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc89e206-3b43-4f23-afe2-59eea509195b",
      "metadata": {
        "id": "bc89e206-3b43-4f23-afe2-59eea509195b"
      },
      "outputs": [],
      "source": [
        "X_avg_custom = create_X_avg_custom(simple_df, wv_custom)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5487419-e32c-46e6-89ff-0cc0927ae0db",
      "metadata": {
        "id": "e5487419-e32c-46e6-89ff-0cc0927ae0db"
      },
      "outputs": [],
      "source": [
        "# Split two different train test - one for pretrained google word2vec and one for our custom word2vec\n",
        "X_train_pretrained, X_test_pretrained, Y_train_pretrained, Y_test_pretrained = train_test_split(X_avg_pretrained, simple_df['sentiment'], test_size=0.2, random_state=48)\n",
        "X_train_custom, X_test_custom, Y_train_custom, Y_test_custom = train_test_split(X_avg_custom, simple_df['sentiment'], test_size=0.2, random_state=48)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59cbaa67-1725-4133-b0fb-1dde152a5c9a",
      "metadata": {
        "id": "59cbaa67-1725-4133-b0fb-1dde152a5c9a",
        "outputId": "f9a056df-4475-4787-839f-8977590b0d4f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Perceptron Results on pretrained model: \n",
            "\n",
            "Accuracy on Testing data: 0.72\n"
          ]
        }
      ],
      "source": [
        "perceptron_model_pretrained = Perceptron(penalty='elasticnet', alpha=0.000001, random_state=200)\n",
        "\n",
        "# Train the model on the training data\n",
        "perceptron_model_pretrained.fit(X_train_pretrained, Y_train_pretrained)\n",
        "\n",
        "# Make predictions on the test data\n",
        "Y_test_pred_pretrained = perceptron_model_pretrained.predict(X_test_pretrained)\n",
        "\n",
        "print('Perceptron Results on pretrained model: ')\n",
        "\n",
        "# Print accuracy\n",
        "print(\"\\nAccuracy on Testing data:\", round(accuracy_score(Y_test_pretrained, Y_test_pred_pretrained), 2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4bbb8164-a03d-4e09-aff0-34620d6277ee",
      "metadata": {
        "id": "4bbb8164-a03d-4e09-aff0-34620d6277ee",
        "outputId": "44f1f73b-872e-4fc8-8505-2708d735375a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Perceptron Results on custom model: \n",
            "\n",
            "Accuracy on Testing data: 0.8\n"
          ]
        }
      ],
      "source": [
        "perceptron_model_custom = Perceptron(penalty='elasticnet', alpha=0.000001, random_state=200)\n",
        "\n",
        "# Train the model on the training data\n",
        "perceptron_model_custom.fit(X_train_custom, Y_train_custom)\n",
        "\n",
        "# Make predictions on the test data\n",
        "Y_test_pred_custom = perceptron_model_custom.predict(X_test_custom)\n",
        "\n",
        "print('Perceptron Results on custom model: ')\n",
        "\n",
        "# Print accuracy\n",
        "print(\"\\nAccuracy on Testing data:\", round(accuracy_score(Y_test_custom, Y_test_pred_custom), 2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4386529b-ad97-422d-8076-48a47778fc7d",
      "metadata": {
        "id": "4386529b-ad97-422d-8076-48a47778fc7d",
        "outputId": "3137834c-c3ce-477d-ae05-771896761f57"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SVM Results pretrained: \n",
            "\n",
            "Accuracy on Testing data: 0.83\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/priyamvora/opt/miniconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "svc_pretrained = LinearSVC(penalty='l1',dual='auto', C=0.2)\n",
        "svc_pretrained = svc_pretrained.fit(X_train_pretrained, Y_train_pretrained)\n",
        "\n",
        "Y_test_pred_svc_pretrained = svc_pretrained.predict(X_test_pretrained)\n",
        "\n",
        "print('SVM Results pretrained: ')\n",
        "\n",
        "\n",
        "# Print accuracy\n",
        "print(\"\\nAccuracy on Testing data:\", round(accuracy_score(Y_test_pretrained, Y_test_pred_svc_pretrained), 2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "070f57d7-19c3-474d-b3e3-5ec9f3bdcd29",
      "metadata": {
        "id": "070f57d7-19c3-474d-b3e3-5ec9f3bdcd29",
        "outputId": "9e364b35-675a-4ad6-fb6a-d5d3ac3bc7ba"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/priyamvora/opt/miniconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SVM Results custom: \n",
            "\n",
            "Accuracy on Testing data: 0.86\n"
          ]
        }
      ],
      "source": [
        "svc_custom = LinearSVC(penalty='l1',dual='auto', C=0.2)\n",
        "svc_custom = svc_pretrained.fit(X_train_custom, Y_train_custom)\n",
        "\n",
        "Y_test_pred_svc_custom = svc_custom.predict(X_test_custom)\n",
        "\n",
        "print('SVM Results custom: ')\n",
        "\n",
        "\n",
        "# Print accuracy\n",
        "print(\"\\nAccuracy on Testing data:\", round(accuracy_score(Y_test_custom, Y_test_pred_svc_custom), 2))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cda1bfcb-9d69-419a-a873-b6fd4e7f1d6b",
      "metadata": {
        "id": "cda1bfcb-9d69-419a-a873-b6fd4e7f1d6b"
      },
      "source": [
        "# Perceptron\n",
        "Accuracy when using TF-IDF (from HW1) - 0.87 \\\n",
        "Accuracy when using Google pretrained Word2Vec - ~0.72 \\\n",
        "Accuracy when using custom Word2Vec - ~0.79\n",
        "\n",
        "# SVM\n",
        "Accuracy when using TF-IDF (from HW1) - 0.91 \\\n",
        "Accuracy when using Google pretrained Word2Vec - 0.83 \\\n",
        "Accuracy when using custom Word2Vec - 0.86\n",
        "\n",
        "# Conclusion\n",
        "Our custom Word2Vec outperforms Google's pretrained model for both perceptron and SVM. This should be obvious, because even though Google's training set is much larger, it is more general and does not contain just reviews. Where as our dataset is purely reviews and is much more specialised and hence can capture word semantics much better. \\\n",
        "TF-IDF performs much better than Word2Vec (both custom and pre trained) for both the models. This is because a lot of words in reviews tend be similar or even the same and TF-IDF captures this particular information (how important a word is in a document) really well compared to Word2Vec"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "673c08f7-aac9-417b-b42c-293fd6a61577",
      "metadata": {
        "id": "673c08f7-aac9-417b-b42c-293fd6a61577"
      },
      "source": [
        "# FFNN using Google Word2Vec Model (Binary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6e7bde5-5b56-442b-b1e2-0e595be96287",
      "metadata": {
        "id": "c6e7bde5-5b56-442b-b1e2-0e595be96287"
      },
      "outputs": [],
      "source": [
        "# Following mnist tutorial. Splitting the raw data first\n",
        "X_train_raw_binary, X_test_raw_binary, Y_train_raw_binary, Y_test_raw_binary = train_test_split(simple_df['review_body'], simple_df['sentiment'], test_size=0.2, random_state=48)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6ad59ff-a3c9-4085-a646-0116f88da8f8",
      "metadata": {
        "id": "a6ad59ff-a3c9-4085-a646-0116f88da8f8"
      },
      "outputs": [],
      "source": [
        "class TrainReview(Dataset):\n",
        "    def __init__(self, reviews, sentiment, word2vec_model, type):\n",
        "        self.reviews = reviews\n",
        "        self.sentiment = sentiment\n",
        "        self.word2vec_model = word2vec_model\n",
        "        self.type = type\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.reviews)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        curr_review = self.reviews.iloc[index]\n",
        "        curr_review = curr_review.replace(',', '')\n",
        "        curr_review = curr_review.replace('.', '')\n",
        "        curr_review = curr_review.split()\n",
        "        curr_vect = []\n",
        "\n",
        "        if self.type == \"google\":\n",
        "            for word in curr_review:\n",
        "                if word in self.word2vec_model:\n",
        "                    curr_vect.append(self.word2vec_model[word])\n",
        "        elif self.type == \"custom\":\n",
        "            for word in curr_review:\n",
        "                if word in self.word2vec_model.wv:\n",
        "                    curr_vect.append(self.word2vec_model.wv[word])\n",
        "\n",
        "        if len(curr_vect) == 0:\n",
        "            curr_vect = np.zeros(300, dtype=float)\n",
        "        else:\n",
        "            curr_vect = np.mean(curr_vect, axis=0)\n",
        "\n",
        "        # Convert to pytorch tensor\n",
        "        curr_vect = torch.from_numpy(curr_vect)\n",
        "        sentiment = self.sentiment.iloc[index]\n",
        "\n",
        "        return curr_vect, sentiment\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f92f8f94-c6ce-429c-85cc-320ca050ef41",
      "metadata": {
        "id": "f92f8f94-c6ce-429c-85cc-320ca050ef41"
      },
      "outputs": [],
      "source": [
        "class TestReview(Dataset):\n",
        "    def __init__(self, reviews, sentiment, word2vec_model, type):\n",
        "        self.reviews = reviews\n",
        "        self.sentiment = sentiment\n",
        "        self.word2vec_model = word2vec_model\n",
        "        self.type = type\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.reviews)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        curr_review = self.reviews.iloc[index]\n",
        "        curr_review = curr_review.replace(',', '')\n",
        "        curr_review = curr_review.replace('.', '')\n",
        "        curr_review = curr_review.split()\n",
        "        curr_vect = []\n",
        "        if self.type == \"google\":\n",
        "            for word in curr_review:\n",
        "                if word in self.word2vec_model:\n",
        "                    curr_vect.append(self.word2vec_model[word])\n",
        "        elif self.type == \"custom\":\n",
        "            for word in curr_review:\n",
        "                if word in self.word2vec_model.wv:\n",
        "                    curr_vect.append(self.word2vec_model.wv[word])\n",
        "        if len(curr_vect) == 0:\n",
        "            curr_vect = np.zeros(300, dtype=float)\n",
        "        else:\n",
        "            curr_vect = np.mean(curr_vect, axis=0)\n",
        "\n",
        "        # Convert to pytorch tensor\n",
        "        curr_vect = torch.from_numpy(curr_vect)\n",
        "        sentiment = self.sentiment.iloc[index]\n",
        "\n",
        "        return curr_vect, sentiment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2998d647-898a-4500-a770-19f5077d0025",
      "metadata": {
        "id": "2998d647-898a-4500-a770-19f5077d0025"
      },
      "outputs": [],
      "source": [
        "train_data_avg_google_binary = TrainReview(X_train_raw_binary, Y_train_raw_binary, wv, \"google\")\n",
        "test_data_avg_google_binary = TestReview(X_test_raw_binary, Y_test_raw_binary, wv, \"google\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7c4e3bd-80bf-4841-a624-7da6330a3cda",
      "metadata": {
        "id": "e7c4e3bd-80bf-4841-a624-7da6330a3cda"
      },
      "outputs": [],
      "source": [
        "# how many samples per batch to load\n",
        "batch_size = 100\n",
        "# percentage of training set to use as validation\n",
        "valid_size = 0.2\n",
        "\n",
        "# obtain training indices that will be used for validation\n",
        "num_train = len(train_data_avg_google_binary)\n",
        "indices = list(range(num_train))\n",
        "np.random.shuffle(indices)\n",
        "split = int(np.floor(valid_size * num_train))\n",
        "train_idx, valid_idx = indices[split:], indices[:split]\n",
        "\n",
        "# define samplers for obtaining training and validation batches\n",
        "train_sampler = SubsetRandomSampler(train_idx)\n",
        "valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "\n",
        "# prepare data loaders\n",
        "train_loader = torch.utils.data.DataLoader(train_data_avg_google_binary, batch_size=batch_size, sampler=train_sampler)\n",
        "valid_loader = torch.utils.data.DataLoader(train_data_avg_google_binary, batch_size=batch_size, sampler=valid_sampler)\n",
        "test_loader = torch.utils.data.DataLoader(test_data_avg_google_binary, batch_size=batch_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04b5950b-2a99-45cd-a740-744df087f8cf",
      "metadata": {
        "id": "04b5950b-2a99-45cd-a740-744df087f8cf"
      },
      "outputs": [],
      "source": [
        "class FFNetBinary(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FFNetBinary, self).__init__()\n",
        "        # number of hidden nodes in each layer (512)\n",
        "        hidden_1 = 50\n",
        "        hidden_2 = 10\n",
        "\n",
        "        self.fc1 = nn.Linear(300, hidden_1)\n",
        "\n",
        "        self.fc2 = nn.Linear(hidden_1, hidden_2)\n",
        "\n",
        "        self.fc3 = nn.Linear(hidden_2, 2)\n",
        "        # dropout prevents overfitting of data\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # Flatten the input if it's not already flattened\n",
        "        x = x.to(torch.float32)\n",
        "\n",
        "        # Apply the first linear layer with activation and dropout\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # Apply the second linear layer with activation and dropout\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # Output layer with two units (binary classification)\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a9b40b6-09e0-4369-8661-75b7581ceee9",
      "metadata": {
        "id": "1a9b40b6-09e0-4369-8661-75b7581ceee9",
        "outputId": "903ea18a-50d9-462b-98dc-26247a53e248"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FFNetBinary(\n",
            "  (fc1): Linear(in_features=300, out_features=50, bias=True)\n",
            "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
            "  (fc3): Linear(in_features=10, out_features=2, bias=True)\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "FFNetGoogleBinaryModel = FFNetBinary()\n",
        "print(FFNetGoogleBinaryModel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "198b527e-28fc-4dc2-8a51-664ea2acc291",
      "metadata": {
        "id": "198b527e-28fc-4dc2-8a51-664ea2acc291"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(FFNetGoogleBinaryModel.parameters(), lr=0.007)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea9e969f-e8fe-48b6-a74f-275afe6a3a35",
      "metadata": {
        "id": "ea9e969f-e8fe-48b6-a74f-275afe6a3a35",
        "outputId": "c80aef18-cdf8-4ec0-90f7-9f3846fedd54"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1 \tTraining Loss: 0.425156 \tValidation Loss: 0.378977\n",
            "Validation loss decreased (inf --> 0.378977).  Saving model ...\n",
            "Epoch: 2 \tTraining Loss: 0.392802 \tValidation Loss: 0.377574\n",
            "Validation loss decreased (0.378977 --> 0.377574).  Saving model ...\n",
            "Epoch: 3 \tTraining Loss: 0.385032 \tValidation Loss: 0.360155\n",
            "Validation loss decreased (0.377574 --> 0.360155).  Saving model ...\n",
            "Epoch: 4 \tTraining Loss: 0.377194 \tValidation Loss: 0.359571\n",
            "Validation loss decreased (0.360155 --> 0.359571).  Saving model ...\n",
            "Epoch: 5 \tTraining Loss: 0.371030 \tValidation Loss: 0.353485\n",
            "Validation loss decreased (0.359571 --> 0.353485).  Saving model ...\n",
            "Epoch: 6 \tTraining Loss: 0.368516 \tValidation Loss: 0.358350\n",
            "Epoch: 7 \tTraining Loss: 0.367582 \tValidation Loss: 0.354510\n",
            "Epoch: 8 \tTraining Loss: 0.363359 \tValidation Loss: 0.366122\n",
            "Epoch: 9 \tTraining Loss: 0.360222 \tValidation Loss: 0.363110\n",
            "Epoch: 10 \tTraining Loss: 0.357364 \tValidation Loss: 0.345815\n",
            "Validation loss decreased (0.353485 --> 0.345815).  Saving model ...\n",
            "Epoch: 11 \tTraining Loss: 0.356731 \tValidation Loss: 0.345410\n",
            "Validation loss decreased (0.345815 --> 0.345410).  Saving model ...\n",
            "Epoch: 12 \tTraining Loss: 0.354007 \tValidation Loss: 0.351977\n",
            "Epoch: 13 \tTraining Loss: 0.353613 \tValidation Loss: 0.341694\n",
            "Validation loss decreased (0.345410 --> 0.341694).  Saving model ...\n",
            "Epoch: 14 \tTraining Loss: 0.352041 \tValidation Loss: 0.342234\n",
            "Epoch: 15 \tTraining Loss: 0.351128 \tValidation Loss: 0.341125\n",
            "Validation loss decreased (0.341694 --> 0.341125).  Saving model ...\n",
            "Epoch: 16 \tTraining Loss: 0.348145 \tValidation Loss: 0.344503\n",
            "Epoch: 17 \tTraining Loss: 0.349402 \tValidation Loss: 0.348322\n",
            "Epoch: 18 \tTraining Loss: 0.346964 \tValidation Loss: 0.341635\n",
            "Epoch: 19 \tTraining Loss: 0.343786 \tValidation Loss: 0.342860\n",
            "Epoch: 20 \tTraining Loss: 0.343300 \tValidation Loss: 0.344804\n",
            "Epoch: 21 \tTraining Loss: 0.344299 \tValidation Loss: 0.341534\n",
            "Epoch: 22 \tTraining Loss: 0.343279 \tValidation Loss: 0.344839\n",
            "Epoch: 23 \tTraining Loss: 0.340971 \tValidation Loss: 0.345475\n",
            "Epoch: 24 \tTraining Loss: 0.341855 \tValidation Loss: 0.341400\n",
            "Epoch: 25 \tTraining Loss: 0.339343 \tValidation Loss: 0.339518\n",
            "Validation loss decreased (0.341125 --> 0.339518).  Saving model ...\n",
            "Epoch: 26 \tTraining Loss: 0.339047 \tValidation Loss: 0.341404\n",
            "Epoch: 27 \tTraining Loss: 0.338349 \tValidation Loss: 0.347235\n",
            "Epoch: 28 \tTraining Loss: 0.338139 \tValidation Loss: 0.354236\n",
            "Epoch: 29 \tTraining Loss: 0.337787 \tValidation Loss: 0.340676\n",
            "Epoch: 30 \tTraining Loss: 0.335677 \tValidation Loss: 0.345622\n",
            "Epoch: 31 \tTraining Loss: 0.336522 \tValidation Loss: 0.364883\n",
            "Epoch: 32 \tTraining Loss: 0.335444 \tValidation Loss: 0.350427\n",
            "Epoch: 33 \tTraining Loss: 0.334521 \tValidation Loss: 0.346201\n",
            "Epoch: 34 \tTraining Loss: 0.335188 \tValidation Loss: 0.343887\n",
            "Epoch: 35 \tTraining Loss: 0.333014 \tValidation Loss: 0.344462\n",
            "Epoch: 36 \tTraining Loss: 0.333578 \tValidation Loss: 0.344673\n",
            "Epoch: 37 \tTraining Loss: 0.332645 \tValidation Loss: 0.341752\n",
            "Epoch: 38 \tTraining Loss: 0.331482 \tValidation Loss: 0.343531\n",
            "Epoch: 39 \tTraining Loss: 0.332191 \tValidation Loss: 0.345024\n",
            "Epoch: 40 \tTraining Loss: 0.329772 \tValidation Loss: 0.344074\n",
            "Epoch: 41 \tTraining Loss: 0.330918 \tValidation Loss: 0.340459\n",
            "Epoch: 42 \tTraining Loss: 0.330818 \tValidation Loss: 0.343003\n",
            "Epoch: 43 \tTraining Loss: 0.328284 \tValidation Loss: 0.354036\n",
            "Epoch: 44 \tTraining Loss: 0.328845 \tValidation Loss: 0.341810\n",
            "Epoch: 45 \tTraining Loss: 0.329586 \tValidation Loss: 0.347596\n",
            "Epoch: 46 \tTraining Loss: 0.329119 \tValidation Loss: 0.341192\n",
            "Epoch: 47 \tTraining Loss: 0.328085 \tValidation Loss: 0.342109\n",
            "Epoch: 48 \tTraining Loss: 0.326412 \tValidation Loss: 0.367644\n",
            "Epoch: 49 \tTraining Loss: 0.328171 \tValidation Loss: 0.348977\n",
            "Epoch: 50 \tTraining Loss: 0.327539 \tValidation Loss: 0.344854\n"
          ]
        }
      ],
      "source": [
        "n_epochs = 50\n",
        "\n",
        "# initialize tracker for minimum validation loss\n",
        "valid_loss_min = np.Inf  # set initial \"min\" to infinity\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    train_loss = 0.0\n",
        "    valid_loss = 0.0\n",
        "\n",
        "    FFNetGoogleBinaryModel.train()  # prep model for training\n",
        "    for data, target in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        data = data.float()\n",
        "        output = FFNetGoogleBinaryModel(data)\n",
        "        target = target.long()  # Convert target to torch.long\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item() * data.size(0)\n",
        "\n",
        "    FFNetGoogleBinaryModel.eval()  # prep model for evaluation\n",
        "    for data, target in valid_loader:\n",
        "        data = data.float()\n",
        "        output = FFNetGoogleBinaryModel(data)\n",
        "        target = target.long()  # Convert target to torch.long\n",
        "        loss = criterion(output, target)\n",
        "        valid_loss += loss.item() * data.size(0)\n",
        "\n",
        "    train_loss = train_loss / (len(train_loader) * batch_size)\n",
        "    valid_loss = valid_loss / (len(valid_loader) * batch_size)\n",
        "\n",
        "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
        "        epoch + 1,\n",
        "        train_loss,\n",
        "        valid_loss\n",
        "    ))\n",
        "\n",
        "    if valid_loss <= valid_loss_min:\n",
        "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
        "            valid_loss_min,\n",
        "            valid_loss))\n",
        "        torch.save(FFNetGoogleBinaryModel.state_dict(), 'model.pt')\n",
        "        valid_loss_min = valid_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b86cc9f-548f-481d-8c2b-d5c5fd74c08e",
      "metadata": {
        "id": "7b86cc9f-548f-481d-8c2b-d5c5fd74c08e",
        "outputId": "00855d98-c53d-483b-db3d-87061e0b3b84"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 447,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "FFNetGoogleBinaryModel.load_state_dict(torch.load('model.pt'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf889a22-da8f-4efa-a039-d55839fcc337",
      "metadata": {
        "id": "bf889a22-da8f-4efa-a039-d55839fcc337"
      },
      "outputs": [],
      "source": [
        "test_loader = torch.utils.data.DataLoader(test_data_avg_google_binary, batch_size=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43051040-f020-46e2-8ac3-20b7df29f26b",
      "metadata": {
        "id": "43051040-f020-46e2-8ac3-20b7df29f26b"
      },
      "outputs": [],
      "source": [
        "def predict(model, dataloader):\n",
        "    prediction_list = []\n",
        "    actual_list = []\n",
        "\n",
        "    for i, batch in enumerate(dataloader):\n",
        "        inputs, targets = batch\n",
        "        inputs = inputs.float()\n",
        "        inputs = inputs.to(device)  # Convert inputs to Float if needed\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        prediction_list.append(int(predicted[0]))\n",
        "        actual_list.append(int(targets[0]))\n",
        "\n",
        "    total = 0\n",
        "    for i in range(len(prediction_list)):\n",
        "        if prediction_list[i] == actual_list[i]:\n",
        "            total += 1\n",
        "\n",
        "    accuracy = float(total) / len(prediction_list)\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f145733-eda0-45c5-b359-4434723a6c59",
      "metadata": {
        "id": "6f145733-eda0-45c5-b359-4434723a6c59",
        "outputId": "4dca2765-d5ee-4d86-f380-eb904bc9bfd7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of FNN using average Google Word2Vec vectors (Binary) : 0.8548\n"
          ]
        }
      ],
      "source": [
        "print('Accuracy of FNN using average Google Word2Vec vectors (Binary) :',str(predict(FFNetGoogleBinaryModel, test_loader)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2838e8c8-5483-4ebd-9bcf-92dc2a2b9ea0",
      "metadata": {
        "id": "2838e8c8-5483-4ebd-9bcf-92dc2a2b9ea0"
      },
      "source": [
        "# FFNN Using Custom Word2Vec Model (Binary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c54c9bd-3b10-4992-9f81-94be023550b8",
      "metadata": {
        "id": "4c54c9bd-3b10-4992-9f81-94be023550b8"
      },
      "outputs": [],
      "source": [
        "train_data_avg_custom_binary = TrainReview(X_train_raw_binary, Y_train_raw_binary, wv_custom, \"custom\")\n",
        "test_data_avg_custom_binary = TestReview(X_test_raw_binary, Y_test_raw_binary, wv_custom, \"custom\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f049f59-ea64-4c59-8700-938c091a89db",
      "metadata": {
        "id": "0f049f59-ea64-4c59-8700-938c091a89db"
      },
      "outputs": [],
      "source": [
        "# how many samples per batch to load\n",
        "batch_size = 100\n",
        "# percentage of training set to use as validation\n",
        "valid_size = 0.2\n",
        "\n",
        "# obtain training indices that will be used for validation\n",
        "num_train = len(train_data_avg_custom_binary)\n",
        "indices = list(range(num_train))\n",
        "np.random.shuffle(indices)\n",
        "split = int(np.floor(valid_size * num_train))\n",
        "train_idx, valid_idx = indices[split:], indices[:split]\n",
        "\n",
        "# define samplers for obtaining training and validation batches\n",
        "train_sampler = SubsetRandomSampler(train_idx)\n",
        "valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "\n",
        "# prepare data loaders\n",
        "train_loader = torch.utils.data.DataLoader(train_data_avg_custom_binary, batch_size=batch_size, sampler=train_sampler)\n",
        "valid_loader = torch.utils.data.DataLoader(train_data_avg_custom_binary, batch_size=batch_size, sampler=valid_sampler)\n",
        "test_loader = torch.utils.data.DataLoader(test_data_avg_custom_binary, batch_size=batch_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01182c64-23f2-40e5-884a-a49e4c0cea9f",
      "metadata": {
        "id": "01182c64-23f2-40e5-884a-a49e4c0cea9f",
        "outputId": "de3bea86-04d8-4508-b94d-524a0c8b4470"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FFNetBinary(\n",
            "  (fc1): Linear(in_features=300, out_features=50, bias=True)\n",
            "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
            "  (fc3): Linear(in_features=10, out_features=2, bias=True)\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "FFNetCustomBinary = FFNetBinary()\n",
        "print(FFNetCustomBinary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d69a79cb-33fb-41bc-a4f1-e1d7a4d7577d",
      "metadata": {
        "id": "d69a79cb-33fb-41bc-a4f1-e1d7a4d7577d"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(FFNetCustomBinary.parameters(), lr=0.005)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8145708-a6b3-4583-a45b-a013da7bd09c",
      "metadata": {
        "id": "e8145708-a6b3-4583-a45b-a013da7bd09c",
        "outputId": "bf4e9112-f90d-4c55-ccd6-7f2a4970d6fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1 \tTraining Loss: 0.662923 \tValidation Loss: 0.609698\n",
            "Validation loss decreased (inf --> 0.609698).  Saving model ...\n",
            "Epoch: 2 \tTraining Loss: 0.561073 \tValidation Loss: 0.485508\n",
            "Validation loss decreased (0.609698 --> 0.485508).  Saving model ...\n",
            "Epoch: 3 \tTraining Loss: 0.474919 \tValidation Loss: 0.417222\n",
            "Validation loss decreased (0.485508 --> 0.417222).  Saving model ...\n",
            "Epoch: 4 \tTraining Loss: 0.429091 \tValidation Loss: 0.384521\n",
            "Validation loss decreased (0.417222 --> 0.384521).  Saving model ...\n",
            "Epoch: 5 \tTraining Loss: 0.404059 \tValidation Loss: 0.366905\n",
            "Validation loss decreased (0.384521 --> 0.366905).  Saving model ...\n",
            "Epoch: 6 \tTraining Loss: 0.388166 \tValidation Loss: 0.355093\n",
            "Validation loss decreased (0.366905 --> 0.355093).  Saving model ...\n",
            "Epoch: 7 \tTraining Loss: 0.377824 \tValidation Loss: 0.347460\n",
            "Validation loss decreased (0.355093 --> 0.347460).  Saving model ...\n",
            "Epoch: 8 \tTraining Loss: 0.370564 \tValidation Loss: 0.341855\n",
            "Validation loss decreased (0.347460 --> 0.341855).  Saving model ...\n",
            "Epoch: 9 \tTraining Loss: 0.364001 \tValidation Loss: 0.337176\n",
            "Validation loss decreased (0.341855 --> 0.337176).  Saving model ...\n",
            "Epoch: 10 \tTraining Loss: 0.359072 \tValidation Loss: 0.334057\n",
            "Validation loss decreased (0.337176 --> 0.334057).  Saving model ...\n",
            "Epoch: 11 \tTraining Loss: 0.355477 \tValidation Loss: 0.331346\n",
            "Validation loss decreased (0.334057 --> 0.331346).  Saving model ...\n",
            "Epoch: 12 \tTraining Loss: 0.350328 \tValidation Loss: 0.329316\n",
            "Validation loss decreased (0.331346 --> 0.329316).  Saving model ...\n",
            "Epoch: 13 \tTraining Loss: 0.348374 \tValidation Loss: 0.327478\n",
            "Validation loss decreased (0.329316 --> 0.327478).  Saving model ...\n",
            "Epoch: 14 \tTraining Loss: 0.344336 \tValidation Loss: 0.325865\n",
            "Validation loss decreased (0.327478 --> 0.325865).  Saving model ...\n",
            "Epoch: 15 \tTraining Loss: 0.342659 \tValidation Loss: 0.324043\n",
            "Validation loss decreased (0.325865 --> 0.324043).  Saving model ...\n",
            "Epoch: 16 \tTraining Loss: 0.340605 \tValidation Loss: 0.323592\n",
            "Validation loss decreased (0.324043 --> 0.323592).  Saving model ...\n",
            "Epoch: 17 \tTraining Loss: 0.337944 \tValidation Loss: 0.321943\n",
            "Validation loss decreased (0.323592 --> 0.321943).  Saving model ...\n",
            "Epoch: 18 \tTraining Loss: 0.337174 \tValidation Loss: 0.320769\n",
            "Validation loss decreased (0.321943 --> 0.320769).  Saving model ...\n",
            "Epoch: 19 \tTraining Loss: 0.336117 \tValidation Loss: 0.320195\n",
            "Validation loss decreased (0.320769 --> 0.320195).  Saving model ...\n",
            "Epoch: 20 \tTraining Loss: 0.334051 \tValidation Loss: 0.319129\n",
            "Validation loss decreased (0.320195 --> 0.319129).  Saving model ...\n",
            "Epoch: 21 \tTraining Loss: 0.331702 \tValidation Loss: 0.318359\n",
            "Validation loss decreased (0.319129 --> 0.318359).  Saving model ...\n",
            "Epoch: 22 \tTraining Loss: 0.331612 \tValidation Loss: 0.317099\n",
            "Validation loss decreased (0.318359 --> 0.317099).  Saving model ...\n",
            "Epoch: 23 \tTraining Loss: 0.328369 \tValidation Loss: 0.317132\n",
            "Epoch: 24 \tTraining Loss: 0.328011 \tValidation Loss: 0.315577\n",
            "Validation loss decreased (0.317099 --> 0.315577).  Saving model ...\n",
            "Epoch: 25 \tTraining Loss: 0.328103 \tValidation Loss: 0.315266\n",
            "Validation loss decreased (0.315577 --> 0.315266).  Saving model ...\n",
            "Epoch: 26 \tTraining Loss: 0.325032 \tValidation Loss: 0.315021\n",
            "Validation loss decreased (0.315266 --> 0.315021).  Saving model ...\n",
            "Epoch: 27 \tTraining Loss: 0.323762 \tValidation Loss: 0.313498\n",
            "Validation loss decreased (0.315021 --> 0.313498).  Saving model ...\n",
            "Epoch: 28 \tTraining Loss: 0.323189 \tValidation Loss: 0.312869\n",
            "Validation loss decreased (0.313498 --> 0.312869).  Saving model ...\n",
            "Epoch: 29 \tTraining Loss: 0.322046 \tValidation Loss: 0.312013\n",
            "Validation loss decreased (0.312869 --> 0.312013).  Saving model ...\n",
            "Epoch: 30 \tTraining Loss: 0.321511 \tValidation Loss: 0.311628\n",
            "Validation loss decreased (0.312013 --> 0.311628).  Saving model ...\n",
            "Epoch: 31 \tTraining Loss: 0.319941 \tValidation Loss: 0.310909\n",
            "Validation loss decreased (0.311628 --> 0.310909).  Saving model ...\n",
            "Epoch: 32 \tTraining Loss: 0.319000 \tValidation Loss: 0.310247\n",
            "Validation loss decreased (0.310909 --> 0.310247).  Saving model ...\n",
            "Epoch: 33 \tTraining Loss: 0.317517 \tValidation Loss: 0.309242\n",
            "Validation loss decreased (0.310247 --> 0.309242).  Saving model ...\n",
            "Epoch: 34 \tTraining Loss: 0.317361 \tValidation Loss: 0.308852\n",
            "Validation loss decreased (0.309242 --> 0.308852).  Saving model ...\n",
            "Epoch: 35 \tTraining Loss: 0.315575 \tValidation Loss: 0.308314\n",
            "Validation loss decreased (0.308852 --> 0.308314).  Saving model ...\n",
            "Epoch: 36 \tTraining Loss: 0.314823 \tValidation Loss: 0.307510\n",
            "Validation loss decreased (0.308314 --> 0.307510).  Saving model ...\n",
            "Epoch: 37 \tTraining Loss: 0.314195 \tValidation Loss: 0.307141\n",
            "Validation loss decreased (0.307510 --> 0.307141).  Saving model ...\n",
            "Epoch: 38 \tTraining Loss: 0.312816 \tValidation Loss: 0.307419\n",
            "Epoch: 39 \tTraining Loss: 0.312288 \tValidation Loss: 0.306249\n",
            "Validation loss decreased (0.307141 --> 0.306249).  Saving model ...\n",
            "Epoch: 40 \tTraining Loss: 0.311211 \tValidation Loss: 0.305655\n",
            "Validation loss decreased (0.306249 --> 0.305655).  Saving model ...\n",
            "Epoch: 41 \tTraining Loss: 0.310806 \tValidation Loss: 0.305211\n",
            "Validation loss decreased (0.305655 --> 0.305211).  Saving model ...\n",
            "Epoch: 42 \tTraining Loss: 0.310590 \tValidation Loss: 0.304743\n",
            "Validation loss decreased (0.305211 --> 0.304743).  Saving model ...\n",
            "Epoch: 43 \tTraining Loss: 0.308776 \tValidation Loss: 0.303669\n",
            "Validation loss decreased (0.304743 --> 0.303669).  Saving model ...\n",
            "Epoch: 44 \tTraining Loss: 0.308792 \tValidation Loss: 0.303411\n",
            "Validation loss decreased (0.303669 --> 0.303411).  Saving model ...\n",
            "Epoch: 45 \tTraining Loss: 0.307692 \tValidation Loss: 0.302790\n",
            "Validation loss decreased (0.303411 --> 0.302790).  Saving model ...\n",
            "Epoch: 46 \tTraining Loss: 0.307693 \tValidation Loss: 0.302425\n",
            "Validation loss decreased (0.302790 --> 0.302425).  Saving model ...\n",
            "Epoch: 47 \tTraining Loss: 0.306577 \tValidation Loss: 0.302119\n",
            "Validation loss decreased (0.302425 --> 0.302119).  Saving model ...\n",
            "Epoch: 48 \tTraining Loss: 0.306065 \tValidation Loss: 0.301600\n",
            "Validation loss decreased (0.302119 --> 0.301600).  Saving model ...\n",
            "Epoch: 49 \tTraining Loss: 0.304609 \tValidation Loss: 0.300630\n",
            "Validation loss decreased (0.301600 --> 0.300630).  Saving model ...\n",
            "Epoch: 50 \tTraining Loss: 0.304441 \tValidation Loss: 0.300466\n",
            "Validation loss decreased (0.300630 --> 0.300466).  Saving model ...\n"
          ]
        }
      ],
      "source": [
        "n_epochs = 50\n",
        "\n",
        "# initialize tracker for minimum validation loss\n",
        "valid_loss_min = np.Inf  # set initial \"min\" to infinity\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    train_loss = 0.0\n",
        "    valid_loss = 0.0\n",
        "\n",
        "    FFNetCustomBinary.train()  # prep model for training\n",
        "    for data, target in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        data = data.float()\n",
        "        output = FFNetCustomBinary(data)\n",
        "        target = target.long()  # Convert target to torch.long\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item() * data.size(0)\n",
        "\n",
        "    FFNetCustomBinary.eval()  # prep model for evaluation\n",
        "    for data, target in valid_loader:\n",
        "        data = data.float()\n",
        "        output = FFNetCustomBinary(data)\n",
        "        target = target.long()  # Convert target to torch.long\n",
        "        loss = criterion(output, target)\n",
        "        valid_loss += loss.item() * data.size(0)\n",
        "\n",
        "    train_loss = train_loss / (len(train_loader) * batch_size)\n",
        "    valid_loss = valid_loss / (len(valid_loader) * batch_size)\n",
        "\n",
        "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
        "        epoch + 1,\n",
        "        train_loss,\n",
        "        valid_loss\n",
        "    ))\n",
        "\n",
        "    if valid_loss <= valid_loss_min:\n",
        "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
        "            valid_loss_min,\n",
        "            valid_loss))\n",
        "        torch.save(FFNetCustomBinary.state_dict(), 'FFNetCustomBinary.pt')\n",
        "        valid_loss_min = valid_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f428cbe-e3d9-4d6c-bfb4-2ba753e5a275",
      "metadata": {
        "id": "6f428cbe-e3d9-4d6c-bfb4-2ba753e5a275",
        "outputId": "2f796ef7-9fa9-463b-fda4-2e7a6a454fc0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 456,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "FFNetCustomBinary.load_state_dict(torch.load('FFNetCustomBinary.pt'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09aa0867-ba23-4bf0-bec0-291ef5b161b7",
      "metadata": {
        "id": "09aa0867-ba23-4bf0-bec0-291ef5b161b7"
      },
      "outputs": [],
      "source": [
        "test_loader = torch.utils.data.DataLoader(test_data_avg_custom_binary, batch_size=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10483d6d-bf77-4713-a86f-b4a3cea808b1",
      "metadata": {
        "id": "10483d6d-bf77-4713-a86f-b4a3cea808b1",
        "outputId": "cbb09706-095e-45b0-d2e1-97ef42d85537"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of FNN using average custom Word2Vec vectors (Binary) : 0.8758\n"
          ]
        }
      ],
      "source": [
        "print('Accuracy of FNN using average custom Word2Vec vectors (Binary) :',str(predict(FFNetCustomBinary, test_loader)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60fb477d-80e6-46c0-a68c-02384b16cfdf",
      "metadata": {
        "id": "60fb477d-80e6-46c0-a68c-02384b16cfdf"
      },
      "source": [
        "# FFNN using Google Word2Vec Model (Ternary):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c3dc0d1-b211-449e-9166-457d24f3de89",
      "metadata": {
        "id": "3c3dc0d1-b211-449e-9166-457d24f3de89"
      },
      "outputs": [],
      "source": [
        "X_train_raw_ternary, X_test_raw_ternary, Y_train_raw_ternary, Y_test_raw_ternary = train_test_split(downsized_df['review_body'], downsized_df['sentiment'], test_size=0.2, random_state=48)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24da120a-1721-4230-bd1f-c079330f4a35",
      "metadata": {
        "id": "24da120a-1721-4230-bd1f-c079330f4a35"
      },
      "outputs": [],
      "source": [
        "train_data_avg_google_ternary = TrainReview(X_train_raw_ternary, Y_train_raw_ternary, wv, \"google\")\n",
        "test_data_avg_google_ternary = TestReview(X_test_raw_ternary, Y_test_raw_ternary, wv, \"google\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39bc4a34-6319-4599-a71a-fd7269c5657a",
      "metadata": {
        "id": "39bc4a34-6319-4599-a71a-fd7269c5657a"
      },
      "outputs": [],
      "source": [
        "# how many samples per batch to load\n",
        "batch_size = 100\n",
        "# percentage of training set to use as validation\n",
        "valid_size = 0.2\n",
        "\n",
        "# obtain training indices that will be used for validation\n",
        "num_train = len(train_data_avg_google_ternary)\n",
        "indices = list(range(num_train))\n",
        "np.random.shuffle(indices)\n",
        "split = int(np.floor(valid_size * num_train))\n",
        "train_idx, valid_idx = indices[split:], indices[:split]\n",
        "\n",
        "# define samplers for obtaining training and validation batches\n",
        "train_sampler = SubsetRandomSampler(train_idx)\n",
        "valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "\n",
        "# prepare data loaders\n",
        "train_loader = torch.utils.data.DataLoader(train_data_avg_google_ternary, batch_size=batch_size, sampler=train_sampler)\n",
        "valid_loader = torch.utils.data.DataLoader(train_data_avg_google_ternary, batch_size=batch_size, sampler=valid_sampler)\n",
        "test_loader = torch.utils.data.DataLoader(test_data_avg_google_ternary, batch_size=batch_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4a3ad2c-a7b9-466a-92eb-bb4ad8c88c90",
      "metadata": {
        "id": "e4a3ad2c-a7b9-466a-92eb-bb4ad8c88c90"
      },
      "outputs": [],
      "source": [
        "class FFNetTernary(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FFNetTernary, self).__init__()\n",
        "        # number of hidden nodes in each layer (512)\n",
        "        hidden_1 = 50\n",
        "        hidden_2 = 10\n",
        "\n",
        "        self.fc1 = nn.Linear(300, hidden_1)\n",
        "\n",
        "        self.fc2 = nn.Linear(hidden_1, hidden_2)\n",
        "\n",
        "        self.fc3 = nn.Linear(hidden_2, 3)\n",
        "        # dropout prevents overfitting of data\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # Flatten the input if it's not already flattened\n",
        "        x = x.to(torch.float32)\n",
        "\n",
        "        # Apply the first linear layer with activation and dropout\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # Apply the second linear layer with activation and dropout\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7844db93-9003-4aca-90e6-065396c327c4",
      "metadata": {
        "id": "7844db93-9003-4aca-90e6-065396c327c4",
        "outputId": "f70c9ea8-e0d9-406e-f07f-c96f6e14b6a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FFNetTernary(\n",
            "  (fc1): Linear(in_features=300, out_features=50, bias=True)\n",
            "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
            "  (fc3): Linear(in_features=10, out_features=3, bias=True)\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "FFNetGoogleTernary = FFNetTernary()\n",
        "print(FFNetGoogleTernary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a51d2b15-1623-4ce2-8ac7-ec67c38611d9",
      "metadata": {
        "id": "a51d2b15-1623-4ce2-8ac7-ec67c38611d9"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(FFNetGoogleTernary.parameters(), lr=0.0007)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab140b1a-4378-4b78-b365-dd9c1a229c4d",
      "metadata": {
        "id": "ab140b1a-4378-4b78-b365-dd9c1a229c4d",
        "outputId": "dffdc9f7-f125-4e78-8a85-75e278701bb4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1 \tTraining Loss: 0.850719 \tValidation Loss: 0.777041\n",
            "Validation loss decreased (inf --> 0.777041).  Saving model ...\n",
            "Epoch: 2 \tTraining Loss: 0.788402 \tValidation Loss: 0.760509\n",
            "Validation loss decreased (0.777041 --> 0.760509).  Saving model ...\n",
            "Epoch: 3 \tTraining Loss: 0.773484 \tValidation Loss: 0.750814\n",
            "Validation loss decreased (0.760509 --> 0.750814).  Saving model ...\n",
            "Epoch: 4 \tTraining Loss: 0.766543 \tValidation Loss: 0.747825\n",
            "Validation loss decreased (0.750814 --> 0.747825).  Saving model ...\n",
            "Epoch: 5 \tTraining Loss: 0.759825 \tValidation Loss: 0.740753\n",
            "Validation loss decreased (0.747825 --> 0.740753).  Saving model ...\n",
            "Epoch: 6 \tTraining Loss: 0.753545 \tValidation Loss: 0.747729\n",
            "Epoch: 7 \tTraining Loss: 0.750422 \tValidation Loss: 0.731484\n",
            "Validation loss decreased (0.740753 --> 0.731484).  Saving model ...\n",
            "Epoch: 8 \tTraining Loss: 0.745416 \tValidation Loss: 0.728270\n",
            "Validation loss decreased (0.731484 --> 0.728270).  Saving model ...\n",
            "Epoch: 9 \tTraining Loss: 0.743404 \tValidation Loss: 0.726750\n",
            "Validation loss decreased (0.728270 --> 0.726750).  Saving model ...\n",
            "Epoch: 10 \tTraining Loss: 0.740362 \tValidation Loss: 0.723295\n",
            "Validation loss decreased (0.726750 --> 0.723295).  Saving model ...\n",
            "Epoch: 11 \tTraining Loss: 0.738348 \tValidation Loss: 0.726673\n",
            "Epoch: 12 \tTraining Loss: 0.735343 \tValidation Loss: 0.724784\n",
            "Epoch: 13 \tTraining Loss: 0.731968 \tValidation Loss: 0.724223\n",
            "Epoch: 14 \tTraining Loss: 0.731188 \tValidation Loss: 0.722184\n",
            "Validation loss decreased (0.723295 --> 0.722184).  Saving model ...\n",
            "Epoch: 15 \tTraining Loss: 0.730098 \tValidation Loss: 0.717550\n",
            "Validation loss decreased (0.722184 --> 0.717550).  Saving model ...\n",
            "Epoch: 16 \tTraining Loss: 0.728212 \tValidation Loss: 0.719335\n",
            "Epoch: 17 \tTraining Loss: 0.727221 \tValidation Loss: 0.716894\n",
            "Validation loss decreased (0.717550 --> 0.716894).  Saving model ...\n",
            "Epoch: 18 \tTraining Loss: 0.725522 \tValidation Loss: 0.716576\n",
            "Validation loss decreased (0.716894 --> 0.716576).  Saving model ...\n",
            "Epoch: 19 \tTraining Loss: 0.723841 \tValidation Loss: 0.717447\n",
            "Epoch: 20 \tTraining Loss: 0.724179 \tValidation Loss: 0.717062\n",
            "Epoch: 21 \tTraining Loss: 0.721337 \tValidation Loss: 0.716118\n",
            "Validation loss decreased (0.716576 --> 0.716118).  Saving model ...\n",
            "Epoch: 22 \tTraining Loss: 0.720459 \tValidation Loss: 0.715593\n",
            "Validation loss decreased (0.716118 --> 0.715593).  Saving model ...\n",
            "Epoch: 23 \tTraining Loss: 0.719822 \tValidation Loss: 0.712765\n",
            "Validation loss decreased (0.715593 --> 0.712765).  Saving model ...\n",
            "Epoch: 24 \tTraining Loss: 0.719251 \tValidation Loss: 0.712819\n",
            "Epoch: 25 \tTraining Loss: 0.717439 \tValidation Loss: 0.711134\n",
            "Validation loss decreased (0.712765 --> 0.711134).  Saving model ...\n",
            "Epoch: 26 \tTraining Loss: 0.717284 \tValidation Loss: 0.711788\n",
            "Epoch: 27 \tTraining Loss: 0.715804 \tValidation Loss: 0.711247\n",
            "Epoch: 28 \tTraining Loss: 0.716642 \tValidation Loss: 0.711208\n",
            "Epoch: 29 \tTraining Loss: 0.715548 \tValidation Loss: 0.711055\n",
            "Validation loss decreased (0.711134 --> 0.711055).  Saving model ...\n",
            "Epoch: 30 \tTraining Loss: 0.713194 \tValidation Loss: 0.712452\n",
            "Epoch: 31 \tTraining Loss: 0.712630 \tValidation Loss: 0.715632\n",
            "Epoch: 32 \tTraining Loss: 0.712817 \tValidation Loss: 0.710207\n",
            "Validation loss decreased (0.711055 --> 0.710207).  Saving model ...\n",
            "Epoch: 33 \tTraining Loss: 0.711994 \tValidation Loss: 0.710952\n",
            "Epoch: 34 \tTraining Loss: 0.711156 \tValidation Loss: 0.710062\n",
            "Validation loss decreased (0.710207 --> 0.710062).  Saving model ...\n",
            "Epoch: 35 \tTraining Loss: 0.711104 \tValidation Loss: 0.712274\n",
            "Epoch: 36 \tTraining Loss: 0.710581 \tValidation Loss: 0.714066\n",
            "Epoch: 37 \tTraining Loss: 0.710911 \tValidation Loss: 0.711016\n",
            "Epoch: 38 \tTraining Loss: 0.709531 \tValidation Loss: 0.710519\n",
            "Epoch: 39 \tTraining Loss: 0.708229 \tValidation Loss: 0.709885\n",
            "Validation loss decreased (0.710062 --> 0.709885).  Saving model ...\n",
            "Epoch: 40 \tTraining Loss: 0.707835 \tValidation Loss: 0.709568\n",
            "Validation loss decreased (0.709885 --> 0.709568).  Saving model ...\n",
            "Epoch: 41 \tTraining Loss: 0.707666 \tValidation Loss: 0.707676\n",
            "Validation loss decreased (0.709568 --> 0.707676).  Saving model ...\n",
            "Epoch: 42 \tTraining Loss: 0.707580 \tValidation Loss: 0.708030\n",
            "Epoch: 43 \tTraining Loss: 0.705923 \tValidation Loss: 0.709791\n",
            "Epoch: 44 \tTraining Loss: 0.705349 \tValidation Loss: 0.710863\n",
            "Epoch: 45 \tTraining Loss: 0.705168 \tValidation Loss: 0.709556\n",
            "Epoch: 46 \tTraining Loss: 0.705250 \tValidation Loss: 0.710210\n",
            "Epoch: 47 \tTraining Loss: 0.706023 \tValidation Loss: 0.710402\n",
            "Epoch: 48 \tTraining Loss: 0.703708 \tValidation Loss: 0.708202\n",
            "Epoch: 49 \tTraining Loss: 0.703329 \tValidation Loss: 0.711153\n",
            "Epoch: 50 \tTraining Loss: 0.703655 \tValidation Loss: 0.707721\n"
          ]
        }
      ],
      "source": [
        "n_epochs = 50\n",
        "\n",
        "# initialize tracker for minimum validation loss\n",
        "valid_loss_min = np.Inf  # set initial \"min\" to infinity\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    train_loss = 0.0\n",
        "    valid_loss = 0.0\n",
        "\n",
        "    FFNetGoogleTernary.train()  # prep model for training\n",
        "    for data, target in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        data = data.float()\n",
        "        output = FFNetGoogleTernary(data)\n",
        "        target = target.long()  # Convert target to torch.long\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item() * data.size(0)\n",
        "\n",
        "    FFNetGoogleTernary.eval()  # prep model for evaluation\n",
        "    for data, target in valid_loader:\n",
        "        data = data.float()\n",
        "        output = FFNetGoogleTernary(data)\n",
        "        target = target.long()  # Convert target to torch.long\n",
        "        loss = criterion(output, target)\n",
        "        valid_loss += loss.item() * data.size(0)\n",
        "\n",
        "    train_loss = train_loss / (len(train_loader) * batch_size)\n",
        "    valid_loss = valid_loss / (len(valid_loader) * batch_size)\n",
        "\n",
        "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
        "        epoch + 1,\n",
        "        train_loss,\n",
        "        valid_loss\n",
        "    ))\n",
        "\n",
        "    if valid_loss <= valid_loss_min:\n",
        "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
        "            valid_loss_min,\n",
        "            valid_loss))\n",
        "        torch.save(FFNetGoogleTernary.state_dict(), 'FFNetGoogleTernary.pt')\n",
        "        valid_loss_min = valid_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff480044-74ea-4261-9af2-5bb52b5b5e5e",
      "metadata": {
        "id": "ff480044-74ea-4261-9af2-5bb52b5b5e5e",
        "outputId": "6e5a4538-0f23-47ff-d11d-2454f50244c1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 466,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "FFNetGoogleTernary.load_state_dict(torch.load('FFNetGoogleTernary.pt'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8f29b28-c58f-4e8b-95e5-9925974edd69",
      "metadata": {
        "id": "f8f29b28-c58f-4e8b-95e5-9925974edd69"
      },
      "outputs": [],
      "source": [
        "test_loader = torch.utils.data.DataLoader(test_data_avg_google_ternary, batch_size=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1177c5ec-a6dc-4e8a-8462-1dd2525cafd4",
      "metadata": {
        "id": "1177c5ec-a6dc-4e8a-8462-1dd2525cafd4",
        "outputId": "0ad0ec81-eed0-4f08-ffff-a9e3923650e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of FNN using average Google Word2Vec vectors (Ternary) : 0.69978\n"
          ]
        }
      ],
      "source": [
        "print('Accuracy of FNN using average Google Word2Vec vectors (Ternary) :',str(predict(FFNetGoogleTernary, test_loader)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0ff3a57-c720-4a64-b722-e27e946532f1",
      "metadata": {
        "id": "c0ff3a57-c720-4a64-b722-e27e946532f1"
      },
      "source": [
        "# FFNN using Custom Word2Vec Model (Ternary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e568a97-793f-417b-9508-a546689c592b",
      "metadata": {
        "id": "9e568a97-793f-417b-9508-a546689c592b"
      },
      "outputs": [],
      "source": [
        "train_data_avg_custom_ternary = TrainReview(X_train_raw_ternary, Y_train_raw_ternary, wv_custom, \"custom\")\n",
        "test_data_avg_custom_ternary = TestReview(X_test_raw_ternary, Y_test_raw_ternary, wv_custom, \"custom\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03388185-bb87-4584-9130-608429cc09df",
      "metadata": {
        "id": "03388185-bb87-4584-9130-608429cc09df"
      },
      "outputs": [],
      "source": [
        "# how many samples per batch to load\n",
        "batch_size = 100\n",
        "# percentage of training set to use as validation\n",
        "valid_size = 0.2\n",
        "\n",
        "# obtain training indices that will be used for validation\n",
        "num_train = len(train_data_avg_custom_ternary)\n",
        "indices = list(range(num_train))\n",
        "np.random.shuffle(indices)\n",
        "split = int(np.floor(valid_size * num_train))\n",
        "train_idx, valid_idx = indices[split:], indices[:split]\n",
        "\n",
        "# define samplers for obtaining training and validation batches\n",
        "train_sampler = SubsetRandomSampler(train_idx)\n",
        "valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "\n",
        "# prepare data loaders\n",
        "train_loader = torch.utils.data.DataLoader(train_data_avg_custom_ternary, batch_size=batch_size, sampler=train_sampler)\n",
        "valid_loader = torch.utils.data.DataLoader(train_data_avg_custom_ternary, batch_size=batch_size, sampler=valid_sampler)\n",
        "test_loader = torch.utils.data.DataLoader(test_data_avg_custom_ternary, batch_size=batch_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81f465bd-90f9-44fd-8e1c-e2407ba508d7",
      "metadata": {
        "id": "81f465bd-90f9-44fd-8e1c-e2407ba508d7",
        "outputId": "9cdc41ea-2796-4687-84f4-1e292d7b57db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FFNetTernary(\n",
            "  (fc1): Linear(in_features=300, out_features=50, bias=True)\n",
            "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
            "  (fc3): Linear(in_features=10, out_features=3, bias=True)\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "FFNetCustomTernary = FFNetTernary()\n",
        "print(FFNetCustomTernary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0de3aee9-d16f-4eac-b477-19e78bf81353",
      "metadata": {
        "id": "0de3aee9-d16f-4eac-b477-19e78bf81353"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(FFNetCustomTernary.parameters(), lr=0.005)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69c3002b-2d83-4268-85f1-07e7ef54792b",
      "metadata": {
        "id": "69c3002b-2d83-4268-85f1-07e7ef54792b",
        "outputId": "24af55b4-72d8-420d-98f2-3d745482d867"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1 \tTraining Loss: 1.027750 \tValidation Loss: 0.971297\n",
            "Validation loss decreased (inf --> 0.971297).  Saving model ...\n",
            "Epoch: 2 \tTraining Loss: 0.925748 \tValidation Loss: 0.857305\n",
            "Validation loss decreased (0.971297 --> 0.857305).  Saving model ...\n",
            "Epoch: 3 \tTraining Loss: 0.853395 \tValidation Loss: 0.802508\n",
            "Validation loss decreased (0.857305 --> 0.802508).  Saving model ...\n",
            "Epoch: 4 \tTraining Loss: 0.816546 \tValidation Loss: 0.771337\n",
            "Validation loss decreased (0.802508 --> 0.771337).  Saving model ...\n",
            "Epoch: 5 \tTraining Loss: 0.794933 \tValidation Loss: 0.754074\n",
            "Validation loss decreased (0.771337 --> 0.754074).  Saving model ...\n",
            "Epoch: 6 \tTraining Loss: 0.781182 \tValidation Loss: 0.742689\n",
            "Validation loss decreased (0.754074 --> 0.742689).  Saving model ...\n",
            "Epoch: 7 \tTraining Loss: 0.769515 \tValidation Loss: 0.734337\n",
            "Validation loss decreased (0.742689 --> 0.734337).  Saving model ...\n",
            "Epoch: 8 \tTraining Loss: 0.763547 \tValidation Loss: 0.727912\n",
            "Validation loss decreased (0.734337 --> 0.727912).  Saving model ...\n",
            "Epoch: 9 \tTraining Loss: 0.755838 \tValidation Loss: 0.722665\n",
            "Validation loss decreased (0.727912 --> 0.722665).  Saving model ...\n",
            "Epoch: 10 \tTraining Loss: 0.750634 \tValidation Loss: 0.718301\n",
            "Validation loss decreased (0.722665 --> 0.718301).  Saving model ...\n",
            "Epoch: 11 \tTraining Loss: 0.745976 \tValidation Loss: 0.714712\n",
            "Validation loss decreased (0.718301 --> 0.714712).  Saving model ...\n",
            "Epoch: 12 \tTraining Loss: 0.742142 \tValidation Loss: 0.711686\n",
            "Validation loss decreased (0.714712 --> 0.711686).  Saving model ...\n",
            "Epoch: 13 \tTraining Loss: 0.738938 \tValidation Loss: 0.708437\n",
            "Validation loss decreased (0.711686 --> 0.708437).  Saving model ...\n",
            "Epoch: 14 \tTraining Loss: 0.735298 \tValidation Loss: 0.705822\n",
            "Validation loss decreased (0.708437 --> 0.705822).  Saving model ...\n",
            "Epoch: 15 \tTraining Loss: 0.732605 \tValidation Loss: 0.703501\n",
            "Validation loss decreased (0.705822 --> 0.703501).  Saving model ...\n",
            "Epoch: 16 \tTraining Loss: 0.729208 \tValidation Loss: 0.701480\n",
            "Validation loss decreased (0.703501 --> 0.701480).  Saving model ...\n",
            "Epoch: 17 \tTraining Loss: 0.726118 \tValidation Loss: 0.698721\n",
            "Validation loss decreased (0.701480 --> 0.698721).  Saving model ...\n",
            "Epoch: 18 \tTraining Loss: 0.725462 \tValidation Loss: 0.697854\n",
            "Validation loss decreased (0.698721 --> 0.697854).  Saving model ...\n",
            "Epoch: 19 \tTraining Loss: 0.722272 \tValidation Loss: 0.696223\n",
            "Validation loss decreased (0.697854 --> 0.696223).  Saving model ...\n",
            "Epoch: 20 \tTraining Loss: 0.719183 \tValidation Loss: 0.694677\n",
            "Validation loss decreased (0.696223 --> 0.694677).  Saving model ...\n",
            "Epoch: 21 \tTraining Loss: 0.718146 \tValidation Loss: 0.692759\n",
            "Validation loss decreased (0.694677 --> 0.692759).  Saving model ...\n",
            "Epoch: 22 \tTraining Loss: 0.715959 \tValidation Loss: 0.691564\n",
            "Validation loss decreased (0.692759 --> 0.691564).  Saving model ...\n",
            "Epoch: 23 \tTraining Loss: 0.715283 \tValidation Loss: 0.691241\n",
            "Validation loss decreased (0.691564 --> 0.691241).  Saving model ...\n",
            "Epoch: 24 \tTraining Loss: 0.713649 \tValidation Loss: 0.688998\n",
            "Validation loss decreased (0.691241 --> 0.688998).  Saving model ...\n",
            "Epoch: 25 \tTraining Loss: 0.712509 \tValidation Loss: 0.687357\n",
            "Validation loss decreased (0.688998 --> 0.687357).  Saving model ...\n",
            "Epoch: 26 \tTraining Loss: 0.710740 \tValidation Loss: 0.686797\n",
            "Validation loss decreased (0.687357 --> 0.686797).  Saving model ...\n",
            "Epoch: 27 \tTraining Loss: 0.708350 \tValidation Loss: 0.685777\n",
            "Validation loss decreased (0.686797 --> 0.685777).  Saving model ...\n",
            "Epoch: 28 \tTraining Loss: 0.707423 \tValidation Loss: 0.683990\n",
            "Validation loss decreased (0.685777 --> 0.683990).  Saving model ...\n",
            "Epoch: 29 \tTraining Loss: 0.706080 \tValidation Loss: 0.683475\n",
            "Validation loss decreased (0.683990 --> 0.683475).  Saving model ...\n",
            "Epoch: 30 \tTraining Loss: 0.705921 \tValidation Loss: 0.681896\n",
            "Validation loss decreased (0.683475 --> 0.681896).  Saving model ...\n",
            "Epoch: 31 \tTraining Loss: 0.704220 \tValidation Loss: 0.682391\n",
            "Epoch: 32 \tTraining Loss: 0.701951 \tValidation Loss: 0.681085\n",
            "Validation loss decreased (0.681896 --> 0.681085).  Saving model ...\n",
            "Epoch: 33 \tTraining Loss: 0.701392 \tValidation Loss: 0.680097\n",
            "Validation loss decreased (0.681085 --> 0.680097).  Saving model ...\n",
            "Epoch: 34 \tTraining Loss: 0.700489 \tValidation Loss: 0.678628\n",
            "Validation loss decreased (0.680097 --> 0.678628).  Saving model ...\n",
            "Epoch: 35 \tTraining Loss: 0.700056 \tValidation Loss: 0.678046\n",
            "Validation loss decreased (0.678628 --> 0.678046).  Saving model ...\n",
            "Epoch: 36 \tTraining Loss: 0.698662 \tValidation Loss: 0.677414\n",
            "Validation loss decreased (0.678046 --> 0.677414).  Saving model ...\n",
            "Epoch: 37 \tTraining Loss: 0.696624 \tValidation Loss: 0.676246\n",
            "Validation loss decreased (0.677414 --> 0.676246).  Saving model ...\n",
            "Epoch: 38 \tTraining Loss: 0.696187 \tValidation Loss: 0.675880\n",
            "Validation loss decreased (0.676246 --> 0.675880).  Saving model ...\n",
            "Epoch: 39 \tTraining Loss: 0.695539 \tValidation Loss: 0.675084\n",
            "Validation loss decreased (0.675880 --> 0.675084).  Saving model ...\n",
            "Epoch: 40 \tTraining Loss: 0.695109 \tValidation Loss: 0.674982\n",
            "Validation loss decreased (0.675084 --> 0.674982).  Saving model ...\n",
            "Epoch: 41 \tTraining Loss: 0.694458 \tValidation Loss: 0.673711\n",
            "Validation loss decreased (0.674982 --> 0.673711).  Saving model ...\n",
            "Epoch: 42 \tTraining Loss: 0.691751 \tValidation Loss: 0.673634\n",
            "Validation loss decreased (0.673711 --> 0.673634).  Saving model ...\n",
            "Epoch: 43 \tTraining Loss: 0.692026 \tValidation Loss: 0.672514\n",
            "Validation loss decreased (0.673634 --> 0.672514).  Saving model ...\n",
            "Epoch: 44 \tTraining Loss: 0.690789 \tValidation Loss: 0.672902\n",
            "Epoch: 45 \tTraining Loss: 0.689870 \tValidation Loss: 0.671343\n",
            "Validation loss decreased (0.672514 --> 0.671343).  Saving model ...\n",
            "Epoch: 46 \tTraining Loss: 0.690693 \tValidation Loss: 0.672084\n",
            "Epoch: 47 \tTraining Loss: 0.689084 \tValidation Loss: 0.670760\n",
            "Validation loss decreased (0.671343 --> 0.670760).  Saving model ...\n",
            "Epoch: 48 \tTraining Loss: 0.687453 \tValidation Loss: 0.670066\n",
            "Validation loss decreased (0.670760 --> 0.670066).  Saving model ...\n",
            "Epoch: 49 \tTraining Loss: 0.687221 \tValidation Loss: 0.669159\n",
            "Validation loss decreased (0.670066 --> 0.669159).  Saving model ...\n",
            "Epoch: 50 \tTraining Loss: 0.687098 \tValidation Loss: 0.668719\n",
            "Validation loss decreased (0.669159 --> 0.668719).  Saving model ...\n"
          ]
        }
      ],
      "source": [
        "n_epochs = 50\n",
        "\n",
        "# initialize tracker for minimum validation loss\n",
        "valid_loss_min = np.Inf  # set initial \"min\" to infinity\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    train_loss = 0.0\n",
        "    valid_loss = 0.0\n",
        "\n",
        "    FFNetCustomTernary.train()  # prep model for training\n",
        "    for data, target in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        data = data.float()\n",
        "        output = FFNetCustomTernary(data)\n",
        "        target = target.long()  # Convert target to torch.long\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item() * data.size(0)\n",
        "\n",
        "    FFNetCustomTernary.eval()  # prep model for evaluation\n",
        "    for data, target in valid_loader:\n",
        "        data = data.float()\n",
        "        output = FFNetCustomTernary(data)\n",
        "        target = target.long()  # Convert target to torch.long\n",
        "        loss = criterion(output, target)\n",
        "        valid_loss += loss.item() * data.size(0)\n",
        "\n",
        "    train_loss = train_loss / (len(train_loader) * batch_size)\n",
        "    valid_loss = valid_loss / (len(valid_loader) * batch_size)\n",
        "\n",
        "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
        "        epoch + 1,\n",
        "        train_loss,\n",
        "        valid_loss\n",
        "    ))\n",
        "\n",
        "    if valid_loss <= valid_loss_min:\n",
        "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
        "            valid_loss_min,\n",
        "            valid_loss))\n",
        "        torch.save(FFNetCustomTernary.state_dict(), 'FFNetCustomTernary.pt')\n",
        "        valid_loss_min = valid_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4fd61c65-68b4-4adc-becb-16bc06dbb110",
      "metadata": {
        "id": "4fd61c65-68b4-4adc-becb-16bc06dbb110",
        "outputId": "1c0b9b64-2940-426d-a1d9-63c4dc1da5eb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 474,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "FFNetCustomTernary.load_state_dict(torch.load('FFNetCustomTernary.pt'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55dd34a5-b702-4dd7-83d7-982106ab18e0",
      "metadata": {
        "id": "55dd34a5-b702-4dd7-83d7-982106ab18e0"
      },
      "outputs": [],
      "source": [
        "test_loader = torch.utils.data.DataLoader(test_data_avg_custom_ternary, batch_size=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21d878ac-b8fe-4a0d-b761-04082bce1003",
      "metadata": {
        "id": "21d878ac-b8fe-4a0d-b761-04082bce1003",
        "outputId": "51f9467f-f8ef-4bfb-e3ae-4fd742026bb1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of FNN using average custom Word2Vec vectors (Ternary) : 0.71816\n"
          ]
        }
      ],
      "source": [
        "print('Accuracy of FNN using average custom Word2Vec vectors (Ternary) :',str(predict(FFNetCustomTernary, test_loader)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43c08aec-7a7e-47b8-b954-a44f6e5d6919",
      "metadata": {
        "id": "43c08aec-7a7e-47b8-b954-a44f6e5d6919"
      },
      "source": [
        "# FFNN Concat Google Vectors - (Binary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be3bcb40-3d2b-4f42-bc24-b837174cfca6",
      "metadata": {
        "id": "be3bcb40-3d2b-4f42-bc24-b837174cfca6"
      },
      "outputs": [],
      "source": [
        "X_train_raw_binary, X_test_raw_binary, Y_train_raw_binary, Y_test_raw_binary = train_test_split(simple_df['review_body'], simple_df['sentiment'], test_size=0.2, random_state=48)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6bcf25b1-e786-4947-bf3c-02db4c223215",
      "metadata": {
        "id": "6bcf25b1-e786-4947-bf3c-02db4c223215"
      },
      "outputs": [],
      "source": [
        "class TrainReviewConcatenation(Dataset):\n",
        "    def __init__(self, reviews, sentiment, word2vec_model, type):\n",
        "        self.reviews = reviews\n",
        "        self.sentiment = sentiment\n",
        "        self.word2vec_model = word2vec_model\n",
        "        self.type = type\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.reviews)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        curr_review = self.reviews.iloc[index]\n",
        "        curr_review = curr_review.replace(',', '')\n",
        "        curr_review = curr_review.replace('.', '')\n",
        "        curr_review = curr_review.split()\n",
        "        curr_vect = []\n",
        "        count = 0\n",
        "        if self.type == \"google\":\n",
        "            for word in curr_review:\n",
        "                if count == 10:\n",
        "                    break\n",
        "                if word in self.word2vec_model:\n",
        "                    count+=1\n",
        "                    curr_vect.append(self.word2vec_model[word])\n",
        "        elif self.type == \"custom\":\n",
        "            for word in curr_review:\n",
        "                if count == 10:\n",
        "                    break\n",
        "                if word in self.word2vec_model.wv:\n",
        "                    count+=1\n",
        "                    curr_vect.append(self.word2vec_model.wv[word])\n",
        "        # if review is less than 10 words, append zeros\n",
        "        while count < 10:\n",
        "            curr_vect.append(np.zeros(300, dtype=float))\n",
        "            count+=1\n",
        "        if len(curr_vect) == 0:\n",
        "            curr_vect = np.zeros(3000, dtype=float)\n",
        "        else:\n",
        "            curr_vect = np.array(curr_vect)\n",
        "            curr_vect = curr_vect.flatten()\n",
        "\n",
        "\n",
        "        # Convert to pytorch tensor\n",
        "        curr_vect = torch.from_numpy(curr_vect)\n",
        "        sentiment = self.sentiment.iloc[index]\n",
        "\n",
        "        return curr_vect, sentiment\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a494e543-f07d-4467-acb0-59d18edcbb7e",
      "metadata": {
        "id": "a494e543-f07d-4467-acb0-59d18edcbb7e"
      },
      "outputs": [],
      "source": [
        "class TestReviewConcatenation(Dataset):\n",
        "    def __init__(self, reviews, sentiment, word2vec_model, type):\n",
        "        self.reviews = reviews\n",
        "        self.sentiment = sentiment\n",
        "        self.word2vec_model = word2vec_model\n",
        "        self.type = type\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.reviews)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        curr_review = self.reviews.iloc[index]\n",
        "        curr_review = curr_review.replace(',', '')\n",
        "        curr_review = curr_review.replace('.', '')\n",
        "        curr_review = curr_review.split()\n",
        "        curr_vect = []\n",
        "        count = 0\n",
        "        if self.type == \"google\":\n",
        "            for word in curr_review:\n",
        "                if(count == 10):\n",
        "                    break\n",
        "                if word in self.word2vec_model:\n",
        "                    count+=1\n",
        "                    curr_vect.append(self.word2vec_model[word])\n",
        "        elif self.type == \"custom\":\n",
        "            for word in curr_review:\n",
        "                if(count == 10):\n",
        "                    break\n",
        "                if word in self.word2vec_model.wv:\n",
        "                    count+=1\n",
        "                    curr_vect.append(self.word2vec_model.wv[word])\n",
        "        while count < 10:\n",
        "            curr_vect.append(np.zeros(300, dtype=float))\n",
        "            count+=1\n",
        "        if len(curr_vect) == 0:\n",
        "            curr_vect = np.zeros(3000, dtype=float)\n",
        "        else:\n",
        "            curr_vect = np.array(curr_vect)\n",
        "            curr_vect = curr_vect.flatten()\n",
        "\n",
        "        # Convert to pytorch tensor\n",
        "        curr_vect = torch.from_numpy(curr_vect)\n",
        "        sentiment = self.sentiment.iloc[index]\n",
        "\n",
        "        return curr_vect, sentiment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2ef4372-d020-45da-92d2-096883cad535",
      "metadata": {
        "id": "e2ef4372-d020-45da-92d2-096883cad535"
      },
      "outputs": [],
      "source": [
        "train_data_concat_google_binary = TrainReviewConcatenation(X_train_raw_binary, Y_train_raw_binary, wv, \"google\")\n",
        "test_data_concat_google_binary = TestReviewConcatenation(X_test_raw_binary, Y_test_raw_binary, wv, \"google\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17c1c2ef-870f-4dd2-abc0-fb81e6a20059",
      "metadata": {
        "id": "17c1c2ef-870f-4dd2-abc0-fb81e6a20059",
        "outputId": "dfa80956-f2f3-441d-fdc5-db9eff4ec33b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample shape: torch.Size([3000])\n",
            "Label: 0\n",
            "Sample content: tensor([0.0801, 0.1050, 0.0498,  ..., 0.0325, 0.2793, 0.2451])\n"
          ]
        }
      ],
      "source": [
        "sample_index = 12454\n",
        "sample, label = train_data_concat_google_binary[sample_index]\n",
        "\n",
        "# Print the shape of the sample and its label\n",
        "print(\"Sample shape:\", sample.shape)\n",
        "print(\"Label:\", label)\n",
        "print(\"Sample content:\", sample)\n",
        "\n",
        "# unique_labels = set()\n",
        "# for _, label in train_data_concat_google_binary:\n",
        "#     unique_labels.add(label)\n",
        "\n",
        "# # Print unique labels\n",
        "# print(\"Unique Labels:\", unique_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34ea9e90-e6b0-4b76-90f9-cc33da4152aa",
      "metadata": {
        "id": "34ea9e90-e6b0-4b76-90f9-cc33da4152aa"
      },
      "outputs": [],
      "source": [
        "# how many samples per batch to load\n",
        "batch_size = 100\n",
        "# percentage of training set to use as validation\n",
        "valid_size = 0.2\n",
        "\n",
        "# obtain training indices that will be used for validation\n",
        "num_train = len(train_data_concat_google_binary)\n",
        "indices = list(range(num_train))\n",
        "np.random.shuffle(indices)\n",
        "split = int(np.floor(valid_size * num_train))\n",
        "train_idx, valid_idx = indices[split:], indices[:split]\n",
        "\n",
        "# define samplers for obtaining training and validation batches\n",
        "train_sampler = SubsetRandomSampler(train_idx)\n",
        "valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "\n",
        "# prepare data loaders\n",
        "train_loader = torch.utils.data.DataLoader(train_data_concat_google_binary, batch_size=batch_size, sampler=train_sampler)\n",
        "valid_loader = torch.utils.data.DataLoader(train_data_concat_google_binary, batch_size=batch_size, sampler=valid_sampler)\n",
        "test_loader = torch.utils.data.DataLoader(test_data_concat_google_binary, batch_size=batch_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e696b466-4925-4556-8dcc-72533072144a",
      "metadata": {
        "id": "e696b466-4925-4556-8dcc-72533072144a"
      },
      "outputs": [],
      "source": [
        "class FFNetConcatBinary(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FFNetConcatBinary, self).__init__()\n",
        "        hidden_1 = 50\n",
        "        hidden_2 = 10\n",
        "\n",
        "        self.fc1 = nn.Linear(3000, hidden_1)\n",
        "\n",
        "        self.fc2 = nn.Linear(hidden_1, hidden_2)\n",
        "\n",
        "        self.fc3 = nn.Linear(hidden_2, 2)\n",
        "        # dropout prevents overfitting of data\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # Flatten the input if it's not already flattened\n",
        "        x = x.to(torch.float32)\n",
        "\n",
        "        # Apply the first linear layer with activation and dropout\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # Apply the second linear layer with activation and dropout\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # Output layer with two units (binary classification)\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5cd03d2-e771-41b6-abc5-8247de444b3b",
      "metadata": {
        "id": "f5cd03d2-e771-41b6-abc5-8247de444b3b",
        "outputId": "7034ba64-058b-4f2a-b6c9-21ee3745d4b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FFNetConcatBinary(\n",
            "  (fc1): Linear(in_features=3000, out_features=50, bias=True)\n",
            "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
            "  (fc3): Linear(in_features=10, out_features=2, bias=True)\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "FFNetGoogleBinaryConcatModel = FFNetConcatBinary()\n",
        "print(FFNetGoogleBinaryConcatModel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57383691-d0d4-4f70-9148-29c2bd5009ca",
      "metadata": {
        "id": "57383691-d0d4-4f70-9148-29c2bd5009ca"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(FFNetGoogleBinaryConcatModel.parameters(), lr=0.007)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40dbc568-3af8-4ea8-bafe-b15590c101a0",
      "metadata": {
        "id": "40dbc568-3af8-4ea8-bafe-b15590c101a0",
        "outputId": "0cd1c26c-d7ae-434c-ef72-49cfbde6292a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1 \tTraining Loss: 0.436369 \tValidation Loss: 0.469405\n",
            "Validation loss decreased (inf --> 0.469405).  Saving model ...\n",
            "Epoch: 2 \tTraining Loss: 0.435617 \tValidation Loss: 0.469731\n",
            "Epoch: 3 \tTraining Loss: 0.433587 \tValidation Loss: 0.471212\n",
            "Epoch: 4 \tTraining Loss: 0.433345 \tValidation Loss: 0.469882\n",
            "Epoch: 5 \tTraining Loss: 0.432315 \tValidation Loss: 0.467921\n",
            "Validation loss decreased (0.469405 --> 0.467921).  Saving model ...\n",
            "Epoch: 6 \tTraining Loss: 0.432433 \tValidation Loss: 0.468019\n",
            "Epoch: 7 \tTraining Loss: 0.430994 \tValidation Loss: 0.467667\n",
            "Validation loss decreased (0.467921 --> 0.467667).  Saving model ...\n",
            "Epoch: 8 \tTraining Loss: 0.431140 \tValidation Loss: 0.467769\n",
            "Epoch: 9 \tTraining Loss: 0.430080 \tValidation Loss: 0.467743\n",
            "Epoch: 10 \tTraining Loss: 0.430776 \tValidation Loss: 0.468133\n",
            "Epoch: 11 \tTraining Loss: 0.430516 \tValidation Loss: 0.467035\n",
            "Validation loss decreased (0.467667 --> 0.467035).  Saving model ...\n",
            "Epoch: 12 \tTraining Loss: 0.428352 \tValidation Loss: 0.468996\n",
            "Epoch: 13 \tTraining Loss: 0.429628 \tValidation Loss: 0.466681\n",
            "Validation loss decreased (0.467035 --> 0.466681).  Saving model ...\n",
            "Epoch: 14 \tTraining Loss: 0.428617 \tValidation Loss: 0.467864\n",
            "Epoch: 15 \tTraining Loss: 0.428961 \tValidation Loss: 0.467386\n",
            "Epoch: 16 \tTraining Loss: 0.428661 \tValidation Loss: 0.466928\n",
            "Epoch: 17 \tTraining Loss: 0.428248 \tValidation Loss: 0.467493\n",
            "Epoch: 18 \tTraining Loss: 0.427502 \tValidation Loss: 0.468720\n",
            "Epoch: 19 \tTraining Loss: 0.429241 \tValidation Loss: 0.467625\n",
            "Epoch: 20 \tTraining Loss: 0.428472 \tValidation Loss: 0.467652\n",
            "Epoch: 21 \tTraining Loss: 0.426497 \tValidation Loss: 0.467428\n",
            "Epoch: 22 \tTraining Loss: 0.427710 \tValidation Loss: 0.467199\n",
            "Epoch: 23 \tTraining Loss: 0.428216 \tValidation Loss: 0.467278\n",
            "Epoch: 24 \tTraining Loss: 0.426492 \tValidation Loss: 0.467840\n",
            "Epoch: 25 \tTraining Loss: 0.427829 \tValidation Loss: 0.466985\n",
            "Epoch: 26 \tTraining Loss: 0.427962 \tValidation Loss: 0.465558\n",
            "Validation loss decreased (0.466681 --> 0.465558).  Saving model ...\n",
            "Epoch: 27 \tTraining Loss: 0.426527 \tValidation Loss: 0.468671\n",
            "Epoch: 28 \tTraining Loss: 0.427220 \tValidation Loss: 0.467250\n",
            "Epoch: 29 \tTraining Loss: 0.426937 \tValidation Loss: 0.466111\n",
            "Epoch: 30 \tTraining Loss: 0.425953 \tValidation Loss: 0.468292\n",
            "Epoch: 31 \tTraining Loss: 0.425548 \tValidation Loss: 0.467389\n",
            "Epoch: 32 \tTraining Loss: 0.426759 \tValidation Loss: 0.466901\n",
            "Epoch: 33 \tTraining Loss: 0.425582 \tValidation Loss: 0.467822\n",
            "Epoch: 34 \tTraining Loss: 0.426008 \tValidation Loss: 0.467016\n",
            "Epoch: 35 \tTraining Loss: 0.425398 \tValidation Loss: 0.466894\n",
            "Epoch: 36 \tTraining Loss: 0.425345 \tValidation Loss: 0.468255\n",
            "Epoch: 37 \tTraining Loss: 0.426940 \tValidation Loss: 0.466331\n",
            "Epoch: 38 \tTraining Loss: 0.425767 \tValidation Loss: 0.467496\n",
            "Epoch: 39 \tTraining Loss: 0.425590 \tValidation Loss: 0.467673\n",
            "Epoch: 40 \tTraining Loss: 0.425475 \tValidation Loss: 0.466800\n",
            "Epoch: 41 \tTraining Loss: 0.425314 \tValidation Loss: 0.467897\n",
            "Epoch: 42 \tTraining Loss: 0.424918 \tValidation Loss: 0.467499\n",
            "Epoch: 43 \tTraining Loss: 0.424477 \tValidation Loss: 0.467352\n",
            "Epoch: 44 \tTraining Loss: 0.424461 \tValidation Loss: 0.467548\n",
            "Epoch: 45 \tTraining Loss: 0.424981 \tValidation Loss: 0.466211\n",
            "Epoch: 46 \tTraining Loss: 0.424584 \tValidation Loss: 0.466145\n",
            "Epoch: 47 \tTraining Loss: 0.424372 \tValidation Loss: 0.467455\n",
            "Epoch: 48 \tTraining Loss: 0.424520 \tValidation Loss: 0.467005\n",
            "Epoch: 49 \tTraining Loss: 0.424883 \tValidation Loss: 0.466764\n",
            "Epoch: 50 \tTraining Loss: 0.423841 \tValidation Loss: 0.466962\n"
          ]
        }
      ],
      "source": [
        "n_epochs = 50\n",
        "\n",
        "# initialize tracker for minimum validation loss\n",
        "valid_loss_min = np.Inf  # set initial \"min\" to infinity\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    train_loss = 0.0\n",
        "    valid_loss = 0.0\n",
        "\n",
        "    FFNetGoogleBinaryConcatModel.train()  # prep model for training\n",
        "    for data, target in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        data = data.float()\n",
        "        output = FFNetGoogleBinaryConcatModel(data)\n",
        "        target = target.long()  # Convert target to torch.long\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item() * data.size(0)\n",
        "\n",
        "    FFNetGoogleBinaryConcatModel.eval()  # prep model for evaluation\n",
        "    for data, target in valid_loader:\n",
        "        data = data.float()\n",
        "        output = FFNetGoogleBinaryConcatModel(data)\n",
        "        target = target.long()  # Convert target to torch.long\n",
        "        loss = criterion(output, target)\n",
        "        valid_loss += loss.item() * data.size(0)\n",
        "\n",
        "    train_loss = train_loss / (len(train_loader) * batch_size)\n",
        "    valid_loss = valid_loss / (len(valid_loader) * batch_size)\n",
        "\n",
        "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
        "        epoch + 1,\n",
        "        train_loss,\n",
        "        valid_loss\n",
        "    ))\n",
        "\n",
        "    if valid_loss <= valid_loss_min:\n",
        "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
        "            valid_loss_min,\n",
        "            valid_loss))\n",
        "        torch.save(FFNetGoogleBinaryConcatModel.state_dict(), 'FFNetGoogleBinaryConcatModel.pt')\n",
        "        valid_loss_min = valid_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a880e5e7-5a65-4e3b-9689-6735ff8fb2ac",
      "metadata": {
        "id": "a880e5e7-5a65-4e3b-9689-6735ff8fb2ac",
        "outputId": "6807c989-8707-456a-fa4a-4a963503c243"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 559,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "FFNetGoogleBinaryConcatModel.load_state_dict(torch.load('FFNetGoogleBinaryConcatModel.pt'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "528fb52d-20d1-4ea4-ac84-926319fedd62",
      "metadata": {
        "id": "528fb52d-20d1-4ea4-ac84-926319fedd62"
      },
      "outputs": [],
      "source": [
        "test_loader = torch.utils.data.DataLoader(test_data_concat_google_binary, batch_size=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bfd55456-aa71-467b-ac38-8f1a4c9e3115",
      "metadata": {
        "id": "bfd55456-aa71-467b-ac38-8f1a4c9e3115",
        "outputId": "a374af17-5b2e-4609-8dbc-a75551ef874f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of FNN using concatened Google Word2Vec vectors (Binary) : 0.772575\n"
          ]
        }
      ],
      "source": [
        "print('Accuracy of FNN using concatened Google Word2Vec vectors (Binary) :',str(predict(FFNetGoogleBinaryConcatModel, test_loader)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5198fda9-d3de-40e2-820c-2db881ef9fb9",
      "metadata": {
        "id": "5198fda9-d3de-40e2-820c-2db881ef9fb9"
      },
      "source": [
        "# FNN using concatened Custom Word2Vec vectors (Binary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8925d0b-01b2-43f9-870a-ea4b38cb60ee",
      "metadata": {
        "id": "c8925d0b-01b2-43f9-870a-ea4b38cb60ee"
      },
      "outputs": [],
      "source": [
        "train_data_concat_custom_binary = TrainReviewConcatenation(X_train_raw_binary, Y_train_raw_binary, wv_custom, \"custom\")\n",
        "test_data_concat_custom_binary = TestReviewConcatenation(X_test_raw_binary, Y_test_raw_binary, wv_custom, \"custom\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d27c9c24-379e-4173-9179-62c835b12f8a",
      "metadata": {
        "id": "d27c9c24-379e-4173-9179-62c835b12f8a"
      },
      "outputs": [],
      "source": [
        "# how many samples per batch to load\n",
        "batch_size = 100\n",
        "# percentage of training set to use as validation\n",
        "valid_size = 0.2\n",
        "\n",
        "# obtain training indices that will be used for validation\n",
        "num_train = len(train_data_concat_custom_binary)\n",
        "indices = list(range(num_train))\n",
        "np.random.shuffle(indices)\n",
        "split = int(np.floor(valid_size * num_train))\n",
        "train_idx, valid_idx = indices[split:], indices[:split]\n",
        "\n",
        "# define samplers for obtaining training and validation batches\n",
        "train_sampler = SubsetRandomSampler(train_idx)\n",
        "valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "\n",
        "# prepare data loaders\n",
        "train_loader = torch.utils.data.DataLoader(train_data_concat_custom_binary, batch_size=batch_size, sampler=train_sampler)\n",
        "valid_loader = torch.utils.data.DataLoader(train_data_concat_custom_binary, batch_size=batch_size, sampler=valid_sampler)\n",
        "test_loader = torch.utils.data.DataLoader(test_data_concat_custom_binary, batch_size=batch_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2bf88c9f-a2e0-4896-9325-c1adced427b2",
      "metadata": {
        "id": "2bf88c9f-a2e0-4896-9325-c1adced427b2",
        "outputId": "e4305316-e6b7-40e5-b090-8773112064ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FFNetConcatBinary(\n",
            "  (fc1): Linear(in_features=3000, out_features=50, bias=True)\n",
            "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
            "  (fc3): Linear(in_features=10, out_features=2, bias=True)\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "FFNetCustomBinaryConcatModel = FFNetConcatBinary()\n",
        "print(FFNetCustomBinaryConcatModel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ddaee87-30ab-46ad-bb46-99d4fe1edd1c",
      "metadata": {
        "id": "7ddaee87-30ab-46ad-bb46-99d4fe1edd1c"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(FFNetCustomBinaryConcatModel.parameters(), lr=0.0007)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41aec816-0b20-4193-ae96-6415c62f55bf",
      "metadata": {
        "id": "41aec816-0b20-4193-ae96-6415c62f55bf",
        "outputId": "bb3c3d2f-d17b-4299-f08c-030070dbc7c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1 \tTraining Loss: 0.360061 \tValidation Loss: 0.465044\n",
            "Validation loss decreased (inf --> 0.465044).  Saving model ...\n",
            "Epoch: 2 \tTraining Loss: 0.358535 \tValidation Loss: 0.464150\n",
            "Validation loss decreased (0.465044 --> 0.464150).  Saving model ...\n",
            "Epoch: 3 \tTraining Loss: 0.359751 \tValidation Loss: 0.466547\n",
            "Epoch: 4 \tTraining Loss: 0.359735 \tValidation Loss: 0.465934\n",
            "Epoch: 5 \tTraining Loss: 0.360517 \tValidation Loss: 0.463393\n",
            "Validation loss decreased (0.464150 --> 0.463393).  Saving model ...\n",
            "Epoch: 6 \tTraining Loss: 0.358409 \tValidation Loss: 0.465918\n",
            "Epoch: 7 \tTraining Loss: 0.359137 \tValidation Loss: 0.466010\n",
            "Epoch: 8 \tTraining Loss: 0.358698 \tValidation Loss: 0.464017\n",
            "Epoch: 9 \tTraining Loss: 0.358866 \tValidation Loss: 0.465587\n",
            "Epoch: 10 \tTraining Loss: 0.358853 \tValidation Loss: 0.462787\n",
            "Validation loss decreased (0.463393 --> 0.462787).  Saving model ...\n",
            "Epoch: 11 \tTraining Loss: 0.359252 \tValidation Loss: 0.462414\n",
            "Validation loss decreased (0.462787 --> 0.462414).  Saving model ...\n",
            "Epoch: 12 \tTraining Loss: 0.358078 \tValidation Loss: 0.466443\n",
            "Epoch: 13 \tTraining Loss: 0.357772 \tValidation Loss: 0.464739\n",
            "Epoch: 14 \tTraining Loss: 0.358290 \tValidation Loss: 0.461487\n",
            "Validation loss decreased (0.462414 --> 0.461487).  Saving model ...\n",
            "Epoch: 15 \tTraining Loss: 0.357595 \tValidation Loss: 0.466390\n",
            "Epoch: 16 \tTraining Loss: 0.357716 \tValidation Loss: 0.465034\n",
            "Epoch: 17 \tTraining Loss: 0.358056 \tValidation Loss: 0.464865\n",
            "Epoch: 18 \tTraining Loss: 0.357960 \tValidation Loss: 0.465028\n",
            "Epoch: 19 \tTraining Loss: 0.356560 \tValidation Loss: 0.467048\n",
            "Epoch: 20 \tTraining Loss: 0.357760 \tValidation Loss: 0.461340\n",
            "Validation loss decreased (0.461487 --> 0.461340).  Saving model ...\n",
            "Epoch: 21 \tTraining Loss: 0.357060 \tValidation Loss: 0.463588\n",
            "Epoch: 22 \tTraining Loss: 0.357281 \tValidation Loss: 0.460791\n",
            "Validation loss decreased (0.461340 --> 0.460791).  Saving model ...\n",
            "Epoch: 23 \tTraining Loss: 0.357399 \tValidation Loss: 0.459820\n",
            "Validation loss decreased (0.460791 --> 0.459820).  Saving model ...\n",
            "Epoch: 24 \tTraining Loss: 0.356870 \tValidation Loss: 0.462820\n",
            "Epoch: 25 \tTraining Loss: 0.357429 \tValidation Loss: 0.462415\n",
            "Epoch: 26 \tTraining Loss: 0.357721 \tValidation Loss: 0.463976\n",
            "Epoch: 27 \tTraining Loss: 0.357417 \tValidation Loss: 0.469554\n",
            "Epoch: 28 \tTraining Loss: 0.357042 \tValidation Loss: 0.466639\n",
            "Epoch: 29 \tTraining Loss: 0.357076 \tValidation Loss: 0.464867\n",
            "Epoch: 30 \tTraining Loss: 0.357853 \tValidation Loss: 0.464616\n",
            "Epoch: 31 \tTraining Loss: 0.355511 \tValidation Loss: 0.461585\n",
            "Epoch: 32 \tTraining Loss: 0.356846 \tValidation Loss: 0.461759\n",
            "Epoch: 33 \tTraining Loss: 0.356247 \tValidation Loss: 0.463668\n",
            "Epoch: 34 \tTraining Loss: 0.357450 \tValidation Loss: 0.459796\n",
            "Validation loss decreased (0.459820 --> 0.459796).  Saving model ...\n",
            "Epoch: 35 \tTraining Loss: 0.356279 \tValidation Loss: 0.465278\n",
            "Epoch: 36 \tTraining Loss: 0.357223 \tValidation Loss: 0.466394\n",
            "Epoch: 37 \tTraining Loss: 0.357025 \tValidation Loss: 0.462619\n",
            "Epoch: 38 \tTraining Loss: 0.355440 \tValidation Loss: 0.457490\n",
            "Validation loss decreased (0.459796 --> 0.457490).  Saving model ...\n",
            "Epoch: 39 \tTraining Loss: 0.356542 \tValidation Loss: 0.467009\n",
            "Epoch: 40 \tTraining Loss: 0.356572 \tValidation Loss: 0.464018\n",
            "Epoch: 41 \tTraining Loss: 0.356148 \tValidation Loss: 0.464883\n",
            "Epoch: 42 \tTraining Loss: 0.355423 \tValidation Loss: 0.467387\n",
            "Epoch: 43 \tTraining Loss: 0.357305 \tValidation Loss: 0.462930\n",
            "Epoch: 44 \tTraining Loss: 0.355522 \tValidation Loss: 0.470131\n",
            "Epoch: 45 \tTraining Loss: 0.355398 \tValidation Loss: 0.465383\n",
            "Epoch: 46 \tTraining Loss: 0.355321 \tValidation Loss: 0.463648\n",
            "Epoch: 47 \tTraining Loss: 0.355409 \tValidation Loss: 0.460911\n",
            "Epoch: 48 \tTraining Loss: 0.357075 \tValidation Loss: 0.463527\n",
            "Epoch: 49 \tTraining Loss: 0.356077 \tValidation Loss: 0.468626\n",
            "Epoch: 50 \tTraining Loss: 0.356396 \tValidation Loss: 0.468189\n"
          ]
        }
      ],
      "source": [
        "n_epochs = 50\n",
        "\n",
        "# initialize tracker for minimum validation loss\n",
        "valid_loss_min = np.Inf  # set initial \"min\" to infinity\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    train_loss = 0.0\n",
        "    valid_loss = 0.0\n",
        "\n",
        "    FFNetCustomBinaryConcatModel.train()  # prep model for training\n",
        "    for data, target in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        data = data.float()\n",
        "        output = FFNetCustomBinaryConcatModel(data)\n",
        "        target = target.long()  # Convert target to torch.long\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item() * data.size(0)\n",
        "\n",
        "    FFNetCustomBinaryConcatModel.eval()  # prep model for evaluation\n",
        "    for data, target in valid_loader:\n",
        "        data = data.float()\n",
        "        output = FFNetCustomBinaryConcatModel(data)\n",
        "        target = target.long()  # Convert target to torch.long\n",
        "        loss = criterion(output, target)\n",
        "        valid_loss += loss.item() * data.size(0)\n",
        "\n",
        "    train_loss = train_loss / (len(train_loader) * batch_size)\n",
        "    valid_loss = valid_loss / (len(valid_loader) * batch_size)\n",
        "\n",
        "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
        "        epoch + 1,\n",
        "        train_loss,\n",
        "        valid_loss\n",
        "    ))\n",
        "\n",
        "    if valid_loss <= valid_loss_min:\n",
        "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
        "            valid_loss_min,\n",
        "            valid_loss))\n",
        "        torch.save(FFNetCustomBinaryConcatModel.state_dict(), 'FFNetCustomBinaryConcatModel.pt')\n",
        "        valid_loss_min = valid_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "367951ff-ecf4-4aab-aee7-0bc8cd50edc0",
      "metadata": {
        "id": "367951ff-ecf4-4aab-aee7-0bc8cd50edc0",
        "outputId": "2455bfe9-fefb-4e9c-f388-20289f9a3097"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 587,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "FFNetCustomBinaryConcatModel.load_state_dict(torch.load('FFNetCustomBinaryConcatModel.pt'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "899b25db-7751-4dd1-8b4f-3e789cd2e842",
      "metadata": {
        "id": "899b25db-7751-4dd1-8b4f-3e789cd2e842"
      },
      "outputs": [],
      "source": [
        "test_loader = torch.utils.data.DataLoader(test_data_concat_custom_binary, batch_size=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac85555b-d13e-4e4a-b424-67e58f8af18d",
      "metadata": {
        "id": "ac85555b-d13e-4e4a-b424-67e58f8af18d",
        "outputId": "6ae87b10-b079-4c1a-a290-066661544484"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of FNN using concatened Custom Word2Vec vectors (Binary) : 0.7847\n"
          ]
        }
      ],
      "source": [
        "print('Accuracy of FNN using concatened Custom Word2Vec vectors (Binary) :',str(predict(FFNetCustomBinaryConcatModel, test_loader)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f57aba04-a6a5-4651-88e2-4acda08293e2",
      "metadata": {
        "id": "f57aba04-a6a5-4651-88e2-4acda08293e2"
      },
      "source": [
        "# FFNN using Google Concat vectors (Ternary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5503a958-7253-4364-bb61-43d925dd0315",
      "metadata": {
        "id": "5503a958-7253-4364-bb61-43d925dd0315"
      },
      "outputs": [],
      "source": [
        "X_train_raw_ternary, X_test_raw_ternary, Y_train_raw_ternary, Y_test_raw_ternary = train_test_split(downsized_df['review_body'], downsized_df['sentiment'], test_size=0.2, random_state=48)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3c9e428-bde8-449b-b33a-550046141a9a",
      "metadata": {
        "id": "f3c9e428-bde8-449b-b33a-550046141a9a"
      },
      "outputs": [],
      "source": [
        "train_data_concat_google_ternary = TrainReviewConcatenation(X_train_raw_ternary, Y_train_raw_ternary, wv, \"google\")\n",
        "test_data_concat_google_ternary = TestReviewConcatenation(X_test_raw_ternary, Y_test_raw_ternary, wv, \"google\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "791217eb-3d3d-4b39-abdb-6ebecf18e8ae",
      "metadata": {
        "id": "791217eb-3d3d-4b39-abdb-6ebecf18e8ae"
      },
      "outputs": [],
      "source": [
        "# how many samples per batch to load\n",
        "batch_size = 100\n",
        "# percentage of training set to use as validation\n",
        "valid_size = 0.2\n",
        "\n",
        "# obtain training indices that will be used for validation\n",
        "num_train = len(train_data_concat_google_ternary)\n",
        "indices = list(range(num_train))\n",
        "np.random.shuffle(indices)\n",
        "split = int(np.floor(valid_size * num_train))\n",
        "train_idx, valid_idx = indices[split:], indices[:split]\n",
        "\n",
        "# define samplers for obtaining training and validation batches\n",
        "train_sampler = SubsetRandomSampler(train_idx)\n",
        "valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "\n",
        "# prepare data loaders\n",
        "train_loader = torch.utils.data.DataLoader(train_data_concat_google_ternary, batch_size=batch_size, sampler=train_sampler)\n",
        "valid_loader = torch.utils.data.DataLoader(train_data_concat_google_ternary, batch_size=batch_size, sampler=valid_sampler)\n",
        "test_loader = torch.utils.data.DataLoader(test_data_concat_google_ternary, batch_size=batch_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2708ae11-0c48-486f-8e32-ba16d8c13260",
      "metadata": {
        "id": "2708ae11-0c48-486f-8e32-ba16d8c13260"
      },
      "outputs": [],
      "source": [
        "class FFNetConcatTernary(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FFNetConcatTernary, self).__init__()\n",
        "        hidden_1 = 50\n",
        "        hidden_2 = 10\n",
        "\n",
        "        self.fc1 = nn.Linear(3000, hidden_1)\n",
        "\n",
        "        self.fc2 = nn.Linear(hidden_1, hidden_2)\n",
        "\n",
        "        self.fc3 = nn.Linear(hidden_2, 3)\n",
        "        # dropout prevents overfitting of data\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # Flatten the input if it's not already flattened\n",
        "        x = x.to(torch.float32)\n",
        "\n",
        "        # Apply the first linear layer with activation and dropout\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # Apply the second linear layer with activation and dropout\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e2ba0cb-bedf-4280-a28d-4187422a0af8",
      "metadata": {
        "id": "2e2ba0cb-bedf-4280-a28d-4187422a0af8",
        "outputId": "db455a4e-434a-4164-952f-458804c5b243"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FFNetConcatTernary(\n",
            "  (fc1): Linear(in_features=3000, out_features=50, bias=True)\n",
            "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
            "  (fc3): Linear(in_features=10, out_features=3, bias=True)\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "FFNetGoogleTernaryConcatModel = FFNetConcatTernary()\n",
        "print(FFNetGoogleTernaryConcatModel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c181bf7-80be-4c9a-890b-81fce867d593",
      "metadata": {
        "id": "7c181bf7-80be-4c9a-890b-81fce867d593"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(FFNetGoogleTernaryConcatModel.parameters(), lr=0.005)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2888ee23-03b6-46f0-97a2-1677a1ce9cd6",
      "metadata": {
        "id": "2888ee23-03b6-46f0-97a2-1677a1ce9cd6",
        "outputId": "ab405820-87a4-4eb8-99a7-a9e0462489f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1 \tTraining Loss: 0.803996 \tValidation Loss: 0.842168\n",
            "Validation loss decreased (inf --> 0.842168).  Saving model ...\n",
            "Epoch: 2 \tTraining Loss: 0.800964 \tValidation Loss: 0.841476\n",
            "Validation loss decreased (0.842168 --> 0.841476).  Saving model ...\n",
            "Epoch: 3 \tTraining Loss: 0.800026 \tValidation Loss: 0.841482\n",
            "Epoch: 4 \tTraining Loss: 0.798978 \tValidation Loss: 0.842239\n",
            "Epoch: 5 \tTraining Loss: 0.798609 \tValidation Loss: 0.840493\n",
            "Validation loss decreased (0.841476 --> 0.840493).  Saving model ...\n",
            "Epoch: 6 \tTraining Loss: 0.797656 \tValidation Loss: 0.841151\n",
            "Epoch: 7 \tTraining Loss: 0.797754 \tValidation Loss: 0.841166\n",
            "Epoch: 8 \tTraining Loss: 0.796667 \tValidation Loss: 0.840628\n",
            "Epoch: 9 \tTraining Loss: 0.796081 \tValidation Loss: 0.840361\n",
            "Validation loss decreased (0.840493 --> 0.840361).  Saving model ...\n",
            "Epoch: 10 \tTraining Loss: 0.796908 \tValidation Loss: 0.840928\n",
            "Epoch: 11 \tTraining Loss: 0.796157 \tValidation Loss: 0.840847\n",
            "Epoch: 12 \tTraining Loss: 0.796553 \tValidation Loss: 0.841099\n",
            "Epoch: 13 \tTraining Loss: 0.795658 \tValidation Loss: 0.840273\n",
            "Validation loss decreased (0.840361 --> 0.840273).  Saving model ...\n",
            "Epoch: 14 \tTraining Loss: 0.795464 \tValidation Loss: 0.840844\n",
            "Epoch: 15 \tTraining Loss: 0.795812 \tValidation Loss: 0.841447\n",
            "Epoch: 16 \tTraining Loss: 0.795471 \tValidation Loss: 0.841348\n",
            "Epoch: 17 \tTraining Loss: 0.796156 \tValidation Loss: 0.840837\n",
            "Epoch: 18 \tTraining Loss: 0.795556 \tValidation Loss: 0.840833\n",
            "Epoch: 19 \tTraining Loss: 0.793910 \tValidation Loss: 0.841392\n",
            "Epoch: 20 \tTraining Loss: 0.794678 \tValidation Loss: 0.840850\n",
            "Epoch: 21 \tTraining Loss: 0.794122 \tValidation Loss: 0.841541\n",
            "Epoch: 22 \tTraining Loss: 0.792863 \tValidation Loss: 0.840905\n",
            "Epoch: 23 \tTraining Loss: 0.793147 \tValidation Loss: 0.841090\n",
            "Epoch: 24 \tTraining Loss: 0.794083 \tValidation Loss: 0.840597\n",
            "Epoch: 25 \tTraining Loss: 0.792907 \tValidation Loss: 0.840847\n",
            "Epoch: 26 \tTraining Loss: 0.791827 \tValidation Loss: 0.840804\n",
            "Epoch: 27 \tTraining Loss: 0.793826 \tValidation Loss: 0.840848\n",
            "Epoch: 28 \tTraining Loss: 0.792726 \tValidation Loss: 0.841222\n",
            "Epoch: 29 \tTraining Loss: 0.793268 \tValidation Loss: 0.841184\n",
            "Epoch: 30 \tTraining Loss: 0.792167 \tValidation Loss: 0.841175\n",
            "Epoch: 31 \tTraining Loss: 0.791434 \tValidation Loss: 0.842134\n",
            "Epoch: 32 \tTraining Loss: 0.792750 \tValidation Loss: 0.840462\n",
            "Epoch: 33 \tTraining Loss: 0.790703 \tValidation Loss: 0.840724\n",
            "Epoch: 34 \tTraining Loss: 0.791404 \tValidation Loss: 0.840527\n",
            "Epoch: 35 \tTraining Loss: 0.791588 \tValidation Loss: 0.840333\n",
            "Epoch: 36 \tTraining Loss: 0.792330 \tValidation Loss: 0.841639\n",
            "Epoch: 37 \tTraining Loss: 0.791326 \tValidation Loss: 0.841630\n",
            "Epoch: 38 \tTraining Loss: 0.791711 \tValidation Loss: 0.840980\n",
            "Epoch: 39 \tTraining Loss: 0.791958 \tValidation Loss: 0.840671\n",
            "Epoch: 40 \tTraining Loss: 0.790517 \tValidation Loss: 0.841689\n",
            "Epoch: 41 \tTraining Loss: 0.790906 \tValidation Loss: 0.841214\n",
            "Epoch: 42 \tTraining Loss: 0.790903 \tValidation Loss: 0.841595\n",
            "Epoch: 43 \tTraining Loss: 0.790574 \tValidation Loss: 0.841236\n",
            "Epoch: 44 \tTraining Loss: 0.792141 \tValidation Loss: 0.841279\n",
            "Epoch: 45 \tTraining Loss: 0.790491 \tValidation Loss: 0.841315\n",
            "Epoch: 46 \tTraining Loss: 0.790482 \tValidation Loss: 0.840841\n",
            "Epoch: 47 \tTraining Loss: 0.790633 \tValidation Loss: 0.841702\n",
            "Epoch: 48 \tTraining Loss: 0.789980 \tValidation Loss: 0.841491\n",
            "Epoch: 49 \tTraining Loss: 0.790509 \tValidation Loss: 0.841717\n",
            "Epoch: 50 \tTraining Loss: 0.789367 \tValidation Loss: 0.841600\n"
          ]
        }
      ],
      "source": [
        "n_epochs = 50\n",
        "\n",
        "# initialize tracker for minimum validation loss\n",
        "valid_loss_min = np.Inf  # set initial \"min\" to infinity\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    train_loss = 0.0\n",
        "    valid_loss = 0.0\n",
        "\n",
        "    FFNetGoogleTernaryConcatModel.train()  # prep model for training\n",
        "    for data, target in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        data = data.float()\n",
        "        output = FFNetGoogleTernaryConcatModel(data)\n",
        "        target = target.long()  # Convert target to torch.long\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item() * data.size(0)\n",
        "\n",
        "    FFNetGoogleTernaryConcatModel.eval()  # prep model for evaluation\n",
        "    for data, target in valid_loader:\n",
        "        data = data.float()\n",
        "        output = FFNetGoogleTernaryConcatModel(data)\n",
        "        target = target.long()  # Convert target to torch.long\n",
        "        loss = criterion(output, target)\n",
        "        valid_loss += loss.item() * data.size(0)\n",
        "\n",
        "    train_loss = train_loss / (len(train_loader) * batch_size)\n",
        "    valid_loss = valid_loss / (len(valid_loader) * batch_size)\n",
        "\n",
        "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
        "        epoch + 1,\n",
        "        train_loss,\n",
        "        valid_loss\n",
        "    ))\n",
        "\n",
        "    if valid_loss <= valid_loss_min:\n",
        "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
        "            valid_loss_min,\n",
        "            valid_loss))\n",
        "        torch.save(FFNetGoogleTernaryConcatModel.state_dict(), 'FFNetGoogleTernaryConcatModel.pt')\n",
        "        valid_loss_min = valid_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5edc296-dba1-4341-bdff-15c9aa50d3b7",
      "metadata": {
        "id": "a5edc296-dba1-4341-bdff-15c9aa50d3b7",
        "outputId": "8b36803b-82f1-4362-9aa5-39cdf41bf24f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 618,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "FFNetGoogleTernaryConcatModel.load_state_dict(torch.load('FFNetGoogleTernaryConcatModel.pt'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9defa016-a41a-46ae-96d2-973793718359",
      "metadata": {
        "id": "9defa016-a41a-46ae-96d2-973793718359"
      },
      "outputs": [],
      "source": [
        "test_loader = torch.utils.data.DataLoader(test_data_concat_google_ternary, batch_size=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3573e180-4172-4e12-8de3-d7e6b749133e",
      "metadata": {
        "id": "3573e180-4172-4e12-8de3-d7e6b749133e",
        "outputId": "466976c1-1e9e-46b4-f54d-a225993cd7fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of FNN using concatened Google Word2Vec vectors (Ternary) : 0.62866\n"
          ]
        }
      ],
      "source": [
        "print('Accuracy of FNN using concatened Google Word2Vec vectors (Ternary) :',str(predict(FFNetGoogleTernaryConcatModel, test_loader)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "854e4ed0-6570-436a-b341-25b17704710e",
      "metadata": {
        "id": "854e4ed0-6570-436a-b341-25b17704710e"
      },
      "source": [
        "# FFNN using Custom Concat vectors (Ternary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4a39b80-55da-491c-a4ea-85e5efbe9add",
      "metadata": {
        "id": "b4a39b80-55da-491c-a4ea-85e5efbe9add"
      },
      "outputs": [],
      "source": [
        "train_data_concat_custom_ternary = TrainReviewConcatenation(X_train_raw_ternary, Y_train_raw_ternary, wv_custom, \"custom\")\n",
        "test_data_concat_custom_ternary = TestReviewConcatenation(X_test_raw_ternary, Y_test_raw_ternary, wv_custom, \"custom\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b205db6-7e3b-4583-a8bb-90596c0c56cb",
      "metadata": {
        "id": "2b205db6-7e3b-4583-a8bb-90596c0c56cb"
      },
      "outputs": [],
      "source": [
        "# how many samples per batch to load\n",
        "batch_size = 100\n",
        "# percentage of training set to use as validation\n",
        "valid_size = 0.2\n",
        "\n",
        "# obtain training indices that will be used for validation\n",
        "num_train = len(train_data_concat_custom_ternary)\n",
        "indices = list(range(num_train))\n",
        "np.random.shuffle(indices)\n",
        "split = int(np.floor(valid_size * num_train))\n",
        "train_idx, valid_idx = indices[split:], indices[:split]\n",
        "\n",
        "# define samplers for obtaining training and validation batches\n",
        "train_sampler = SubsetRandomSampler(train_idx)\n",
        "valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "\n",
        "# prepare data loaders\n",
        "train_loader = torch.utils.data.DataLoader(train_data_concat_custom_ternary, batch_size=batch_size, sampler=train_sampler)\n",
        "valid_loader = torch.utils.data.DataLoader(train_data_concat_custom_ternary, batch_size=batch_size, sampler=valid_sampler)\n",
        "test_loader = torch.utils.data.DataLoader(test_data_concat_custom_ternary, batch_size=batch_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8212ade8-8687-4567-935f-93170cf9aa0d",
      "metadata": {
        "id": "8212ade8-8687-4567-935f-93170cf9aa0d",
        "outputId": "d70255ff-5f32-442d-ed03-e2dc07af6942"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FFNetConcatTernary(\n",
            "  (fc1): Linear(in_features=3000, out_features=50, bias=True)\n",
            "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
            "  (fc3): Linear(in_features=10, out_features=3, bias=True)\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "FFNetCustomTernaryConcatModel = FFNetConcatTernary()\n",
        "print(FFNetCustomTernaryConcatModel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3614fd65-6e74-4c77-a843-48bcfa3dd71a",
      "metadata": {
        "id": "3614fd65-6e74-4c77-a843-48bcfa3dd71a"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(FFNetCustomTernaryConcatModel.parameters(), lr=0.005)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3005e3b9-95e5-4f89-9a5e-d1633e0fc246",
      "metadata": {
        "id": "3005e3b9-95e5-4f89-9a5e-d1633e0fc246",
        "outputId": "ad1e5165-a566-4a56-bf00-333918c87b21"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1 \tTraining Loss: 0.781682 \tValidation Loss: 0.820052\n",
            "Validation loss decreased (inf --> 0.820052).  Saving model ...\n",
            "Epoch: 2 \tTraining Loss: 0.777908 \tValidation Loss: 0.820021\n",
            "Validation loss decreased (0.820052 --> 0.820021).  Saving model ...\n",
            "Epoch: 3 \tTraining Loss: 0.776842 \tValidation Loss: 0.822191\n",
            "Epoch: 4 \tTraining Loss: 0.775477 \tValidation Loss: 0.817134\n",
            "Validation loss decreased (0.820021 --> 0.817134).  Saving model ...\n",
            "Epoch: 5 \tTraining Loss: 0.775500 \tValidation Loss: 0.819469\n",
            "Epoch: 6 \tTraining Loss: 0.776160 \tValidation Loss: 0.816090\n",
            "Validation loss decreased (0.817134 --> 0.816090).  Saving model ...\n",
            "Epoch: 7 \tTraining Loss: 0.773364 \tValidation Loss: 0.816122\n",
            "Epoch: 8 \tTraining Loss: 0.772598 \tValidation Loss: 0.816022\n",
            "Validation loss decreased (0.816090 --> 0.816022).  Saving model ...\n",
            "Epoch: 9 \tTraining Loss: 0.772099 \tValidation Loss: 0.816231\n",
            "Epoch: 10 \tTraining Loss: 0.772981 \tValidation Loss: 0.817808\n",
            "Epoch: 11 \tTraining Loss: 0.772320 \tValidation Loss: 0.818000\n",
            "Epoch: 12 \tTraining Loss: 0.770950 \tValidation Loss: 0.818160\n",
            "Epoch: 13 \tTraining Loss: 0.771379 \tValidation Loss: 0.818371\n",
            "Epoch: 14 \tTraining Loss: 0.771259 \tValidation Loss: 0.817795\n",
            "Epoch: 15 \tTraining Loss: 0.771125 \tValidation Loss: 0.812836\n",
            "Validation loss decreased (0.816022 --> 0.812836).  Saving model ...\n",
            "Epoch: 16 \tTraining Loss: 0.769413 \tValidation Loss: 0.820094\n",
            "Epoch: 17 \tTraining Loss: 0.769531 \tValidation Loss: 0.816487\n",
            "Epoch: 18 \tTraining Loss: 0.768437 \tValidation Loss: 0.812666\n",
            "Validation loss decreased (0.812836 --> 0.812666).  Saving model ...\n",
            "Epoch: 19 \tTraining Loss: 0.769613 \tValidation Loss: 0.817906\n",
            "Epoch: 20 \tTraining Loss: 0.769774 \tValidation Loss: 0.815303\n",
            "Epoch: 21 \tTraining Loss: 0.769552 \tValidation Loss: 0.815309\n",
            "Epoch: 22 \tTraining Loss: 0.768268 \tValidation Loss: 0.815125\n",
            "Epoch: 23 \tTraining Loss: 0.768874 \tValidation Loss: 0.815221\n",
            "Epoch: 24 \tTraining Loss: 0.767660 \tValidation Loss: 0.814234\n",
            "Epoch: 25 \tTraining Loss: 0.767903 \tValidation Loss: 0.816780\n",
            "Epoch: 26 \tTraining Loss: 0.768481 \tValidation Loss: 0.815009\n",
            "Epoch: 27 \tTraining Loss: 0.769091 \tValidation Loss: 0.815921\n",
            "Epoch: 28 \tTraining Loss: 0.767910 \tValidation Loss: 0.812893\n",
            "Epoch: 29 \tTraining Loss: 0.767259 \tValidation Loss: 0.813772\n",
            "Epoch: 30 \tTraining Loss: 0.766777 \tValidation Loss: 0.816121\n",
            "Epoch: 31 \tTraining Loss: 0.766251 \tValidation Loss: 0.816597\n",
            "Epoch: 32 \tTraining Loss: 0.768348 \tValidation Loss: 0.815295\n",
            "Epoch: 33 \tTraining Loss: 0.766035 \tValidation Loss: 0.813394\n",
            "Epoch: 34 \tTraining Loss: 0.766467 \tValidation Loss: 0.819818\n",
            "Epoch: 35 \tTraining Loss: 0.765793 \tValidation Loss: 0.813885\n",
            "Epoch: 36 \tTraining Loss: 0.766670 \tValidation Loss: 0.816081\n",
            "Epoch: 37 \tTraining Loss: 0.765563 \tValidation Loss: 0.815277\n",
            "Epoch: 38 \tTraining Loss: 0.765783 \tValidation Loss: 0.816115\n",
            "Epoch: 39 \tTraining Loss: 0.764768 \tValidation Loss: 0.812937\n",
            "Epoch: 40 \tTraining Loss: 0.765641 \tValidation Loss: 0.814303\n",
            "Epoch: 41 \tTraining Loss: 0.766401 \tValidation Loss: 0.812649\n",
            "Validation loss decreased (0.812666 --> 0.812649).  Saving model ...\n",
            "Epoch: 42 \tTraining Loss: 0.764599 \tValidation Loss: 0.815364\n",
            "Epoch: 43 \tTraining Loss: 0.764935 \tValidation Loss: 0.813379\n",
            "Epoch: 44 \tTraining Loss: 0.764410 \tValidation Loss: 0.816412\n",
            "Epoch: 45 \tTraining Loss: 0.764392 \tValidation Loss: 0.813657\n",
            "Epoch: 46 \tTraining Loss: 0.765055 \tValidation Loss: 0.812537\n",
            "Validation loss decreased (0.812649 --> 0.812537).  Saving model ...\n",
            "Epoch: 47 \tTraining Loss: 0.762962 \tValidation Loss: 0.815505\n",
            "Epoch: 48 \tTraining Loss: 0.763615 \tValidation Loss: 0.813110\n",
            "Epoch: 49 \tTraining Loss: 0.763043 \tValidation Loss: 0.816134\n",
            "Epoch: 50 \tTraining Loss: 0.763754 \tValidation Loss: 0.815527\n"
          ]
        }
      ],
      "source": [
        "n_epochs = 50\n",
        "\n",
        "# initialize tracker for minimum validation loss\n",
        "valid_loss_min = np.Inf  # set initial \"min\" to infinity\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    train_loss = 0.0\n",
        "    valid_loss = 0.0\n",
        "\n",
        "    FFNetCustomTernaryConcatModel.train()  # prep model for training\n",
        "    for data, target in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        data = data.float()\n",
        "        output = FFNetCustomTernaryConcatModel(data)\n",
        "        target = target.long()  # Convert target to torch.long\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item() * data.size(0)\n",
        "\n",
        "    FFNetCustomTernaryConcatModel.eval()  # prep model for evaluation\n",
        "    for data, target in valid_loader:\n",
        "        data = data.float()\n",
        "        output = FFNetCustomTernaryConcatModel(data)\n",
        "        target = target.long()  # Convert target to torch.long\n",
        "        loss = criterion(output, target)\n",
        "        valid_loss += loss.item() * data.size(0)\n",
        "\n",
        "    train_loss = train_loss / (len(train_loader) * batch_size)\n",
        "    valid_loss = valid_loss / (len(valid_loader) * batch_size)\n",
        "\n",
        "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
        "        epoch + 1,\n",
        "        train_loss,\n",
        "        valid_loss\n",
        "    ))\n",
        "\n",
        "    if valid_loss <= valid_loss_min:\n",
        "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
        "            valid_loss_min,\n",
        "            valid_loss))\n",
        "        torch.save(FFNetCustomTernaryConcatModel.state_dict(), 'FFNetCustomTernaryConcatModel.pt')\n",
        "        valid_loss_min = valid_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8b5aa42-9eac-48e1-9784-6aa866689c41",
      "metadata": {
        "id": "d8b5aa42-9eac-48e1-9784-6aa866689c41",
        "outputId": "510272f9-e197-4c7a-af0a-600327bea849"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 636,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "FFNetCustomTernaryConcatModel.load_state_dict(torch.load('FFNetCustomTernaryConcatModel.pt'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f88d9ce-f20c-4638-a7fd-c3ab39d049cb",
      "metadata": {
        "id": "9f88d9ce-f20c-4638-a7fd-c3ab39d049cb"
      },
      "outputs": [],
      "source": [
        "test_loader = torch.utils.data.DataLoader(test_data_concat_custom_ternary, batch_size=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66f6e1a0-58b3-48ff-a272-37c0056a7af9",
      "metadata": {
        "id": "66f6e1a0-58b3-48ff-a272-37c0056a7af9",
        "outputId": "f0d4a844-ecbe-43d8-f213-1a2ff5cdd9ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of FNN using concatened Custom Word2Vec vectors (Ternary) : 0.6348\n"
          ]
        }
      ],
      "source": [
        "print('Accuracy of FNN using concatened Custom Word2Vec vectors (Ternary) :',str(predict(FFNetCustomTernaryConcatModel, test_loader)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5c4dccb-dd4f-4c33-baed-3f1a201f6f5d",
      "metadata": {
        "id": "b5c4dccb-dd4f-4c33-baed-3f1a201f6f5d"
      },
      "source": [
        "# Conclusion\n",
        "We trained a lot of FFNN models in this section. Total of about 8.\n",
        "As expected the Binary Models performed better than Ternary Models.\n",
        "In each of the models, our own model performed relatively better than model trained on Google's dataset. This is obvious because Google data set is not about reviews and more generic. Our dataset is about reviews and hence our models perform better on review classification task\n",
        "\n",
        "## Comparision of Binary Models with Simple Models\n",
        "### When taking average vectors\n",
        "1. FFNN trained on Google vectors and our own vectors performed much better than Perceptron and SVM trained on Google vector and our own vector. This is because of the larger network of hidden layers and nodes, which enables better learning across epochs to classify the data.\n",
        "   \n",
        "### When taking concatenated vectors\n",
        "1. FFNN trained on Google vectors and our own vectors performed almost same as Perceptron and poor compared to SVM trained on Google vector and our own vector. It could be  because as the first 10 words being concatenated do not necessarily have all the information needed to conclude the sentiment of the review\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed8fd6f9-a23f-47d3-a9b7-66186de689db",
      "metadata": {
        "id": "ed8fd6f9-a23f-47d3-a9b7-66186de689db"
      },
      "source": [
        "# CNN Google Vectors (Binary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1793d67f-8953-4f57-ad7a-253dec348815",
      "metadata": {
        "id": "1793d67f-8953-4f57-ad7a-253dec348815"
      },
      "outputs": [],
      "source": [
        "X_train_raw_binary, X_test_raw_binary, Y_train_raw_binary, Y_test_raw_binary = train_test_split(simple_df['review_body'], simple_df['sentiment'], test_size=0.2, random_state=48)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ecd73ba3-462f-454f-95a5-e184d94ad3f2",
      "metadata": {
        "id": "ecd73ba3-462f-454f-95a5-e184d94ad3f2"
      },
      "outputs": [],
      "source": [
        "class TrainReviewCNN(Dataset):\n",
        "    def __init__(self, reviews, sentiment, word2vec_model, type, max_length=50, vector_size=300):\n",
        "        self.reviews = reviews\n",
        "        self.sentiment = sentiment\n",
        "        self.word2vec_model = word2vec_model\n",
        "        self.type = type\n",
        "        self.max_length = max_length\n",
        "        self.vector_size = vector_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.reviews)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        curr_review = self.reviews.iloc[index]\n",
        "        curr_review = curr_review.replace(',', '')\n",
        "        curr_review = curr_review.replace('.', '')\n",
        "        curr_review = curr_review.split()\n",
        "        curr_vect = []\n",
        "        count = 0\n",
        "        if self.type == \"google\":\n",
        "            for word in curr_review:\n",
        "                if count == self.max_length:\n",
        "                    break\n",
        "                if word in self.word2vec_model:\n",
        "                    count += 1\n",
        "                    curr_vect.append(self.word2vec_model[word])\n",
        "        elif self.type == \"custom\":\n",
        "            for word in curr_review:\n",
        "                if count == self.max_length:\n",
        "                    break\n",
        "                if word in self.word2vec_model.wv:\n",
        "                    count += 1\n",
        "                    curr_vect.append(self.word2vec_model.wv[word])\n",
        "        # if review is less than max_length words, append zeros\n",
        "        while count < self.max_length:\n",
        "            curr_vect.append(np.zeros(self.vector_size, dtype=float))\n",
        "            count += 1\n",
        "        if len(curr_vect) == 0:\n",
        "            curr_vect = np.zeros([self.max_length, self.vector_size], dtype=float)\n",
        "        else:\n",
        "            curr_vect = np.array(curr_vect)\n",
        "            # curr_vect = curr_vect.flatten()\n",
        "        curr_vect = np.transpose(curr_vect)\n",
        "        # Convert to pytorch tensor\n",
        "        curr_vect = torch.from_numpy(curr_vect)\n",
        "        sentiment = self.sentiment.iloc[index]\n",
        "\n",
        "        return curr_vect, sentiment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20a48607-d2d5-4974-acbf-c56f990240ab",
      "metadata": {
        "id": "20a48607-d2d5-4974-acbf-c56f990240ab"
      },
      "outputs": [],
      "source": [
        "class TestReviewCNN(Dataset):\n",
        "    def __init__(self, reviews, sentiment, word2vec_model, type, max_length=50, vector_size=300):\n",
        "        self.reviews = reviews\n",
        "        self.sentiment = sentiment\n",
        "        self.word2vec_model = word2vec_model\n",
        "        self.type = type\n",
        "        self.max_length = max_length\n",
        "        self.vector_size = vector_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.reviews)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        curr_review = self.reviews.iloc[index]\n",
        "        curr_review = curr_review.replace(',', '')\n",
        "        curr_review = curr_review.replace('.', '')\n",
        "        curr_review = curr_review.split()\n",
        "        curr_vect = []\n",
        "        count = 0\n",
        "        if self.type == \"google\":\n",
        "            for word in curr_review:\n",
        "                if count == self.max_length:\n",
        "                    break\n",
        "                if word in self.word2vec_model:\n",
        "                    count += 1\n",
        "                    curr_vect.append(self.word2vec_model[word])\n",
        "        elif self.type == \"custom\":\n",
        "            for word in curr_review:\n",
        "                if count == self.max_length:\n",
        "                    break\n",
        "                if word in self.word2vec_model.wv:\n",
        "                    count += 1\n",
        "                    curr_vect.append(self.word2vec_model.wv[word])\n",
        "        # if review is less than max_length words, append zeros\n",
        "        while count < self.max_length:\n",
        "            curr_vect.append(np.zeros(self.vector_size, dtype=float))\n",
        "            count += 1\n",
        "        if len(curr_vect) == 0:\n",
        "            curr_vect = np.zeros([self.max_length,self.vector_size], dtype=float)\n",
        "        else:\n",
        "            curr_vect = np.array(curr_vect)\n",
        "            # curr_vect = curr_vect.flatten()\n",
        "        curr_vect = np.transpose(curr_vect)\n",
        "        # Convert to pytorch tensor\n",
        "        curr_vect = torch.from_numpy(curr_vect)\n",
        "        sentiment = self.sentiment.iloc[index]\n",
        "\n",
        "        return curr_vect, sentiment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c7a53a8-ffcc-4adf-9c65-680650863e90",
      "metadata": {
        "id": "5c7a53a8-ffcc-4adf-9c65-680650863e90"
      },
      "outputs": [],
      "source": [
        "train_data_cnn_google_binary = TrainReviewCNN(X_train_raw_binary, Y_train_raw_binary, wv, \"google\")\n",
        "test_data_cnn_google_binary = TestReviewCNN(X_test_raw_binary, Y_test_raw_binary, wv, \"google\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80033204-92e8-4c94-893b-cac6cb3f398b",
      "metadata": {
        "id": "80033204-92e8-4c94-893b-cac6cb3f398b",
        "outputId": "85327c32-2e8f-412b-c37a-4ca8c835c96b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample shape: torch.Size([50, 300])\n",
            "Label: 0\n",
            "Sample content: tensor([[ 0.0801,  0.1050,  0.0498,  ...,  0.0037,  0.0476, -0.0688],\n",
            "        [-0.1021, -0.0603, -0.1123,  ...,  0.0216, -0.0095, -0.1523],\n",
            "        [-0.0199, -0.0237,  0.0767,  ..., -0.0786,  0.0952, -0.2451],\n",
            "        ...,\n",
            "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
            "       dtype=torch.float64)\n"
          ]
        }
      ],
      "source": [
        "sample_index = 12454\n",
        "sample, label = train_data_cnn_google_binary[sample_index]\n",
        "\n",
        "# Print the shape of the sample and its label\n",
        "print(\"Sample shape:\", sample.shape)\n",
        "print(\"Label:\", label)\n",
        "print(\"Sample content:\", sample)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35a81c02-e9aa-49db-a688-76fa1929673d",
      "metadata": {
        "id": "35a81c02-e9aa-49db-a688-76fa1929673d"
      },
      "outputs": [],
      "source": [
        "# how many samples per batch to load\n",
        "batch_size = 100\n",
        "# percentage of training set to use as validation\n",
        "valid_size = 0.2\n",
        "\n",
        "# obtain training indices that will be used for validation\n",
        "num_train = len(train_data_cnn_google_binary)\n",
        "indices = list(range(num_train))\n",
        "np.random.shuffle(indices)\n",
        "split = int(np.floor(valid_size * num_train))\n",
        "train_idx, valid_idx = indices[split:], indices[:split]\n",
        "\n",
        "# define samplers for obtaining training and validation batches\n",
        "train_sampler = SubsetRandomSampler(train_idx)\n",
        "valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "\n",
        "# prepare data loaders\n",
        "train_loader = torch.utils.data.DataLoader(train_data_cnn_google_binary, batch_size=batch_size, sampler=train_sampler)\n",
        "valid_loader = torch.utils.data.DataLoader(train_data_cnn_google_binary, batch_size=batch_size, sampler=valid_sampler)\n",
        "test_loader = torch.utils.data.DataLoader(test_data_cnn_google_binary, batch_size=batch_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6e536e0-3cb4-48a0-8360-d184a229519d",
      "metadata": {
        "id": "a6e536e0-3cb4-48a0-8360-d184a229519d"
      },
      "outputs": [],
      "source": [
        "class BinaryCNN(nn.Module):\n",
        "    def __init__(self, output_channels1=50, output_channels2=10, max_length=50, vector_size=300):\n",
        "        super(BinaryCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(in_channels=vector_size, out_channels=output_channels1, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv1d(in_channels=output_channels1, out_channels=output_channels2, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool1d(2)\n",
        "\n",
        "        self.fc1 = nn.Linear(120, 2)\n",
        "\n",
        "        self.dropout1 = nn.Dropout(0.3)\n",
        "        self.dropout2 = nn.Dropout(0.3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.pool(self.conv1(x)))\n",
        "        x = self.dropout1(x)\n",
        "        x = F.relu(self.pool(self.conv2(x)))\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        return x\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "GoogleBinaryCNN = BinaryCNN().to(device)\n",
        "print(GoogleBinaryCNN)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xGxgMXXEho5n",
        "outputId": "cbbdf748-f496-42ab-deb9-28c31cc095fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BinaryCNN(\n",
            "  (conv1): Conv1d(300, 50, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "  (conv2): Conv1d(50, 10, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "  (pool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (fc1): Linear(in_features=120, out_features=2, bias=True)\n",
            "  (dropout1): Dropout(p=0.3, inplace=False)\n",
            "  (dropout2): Dropout(p=0.3, inplace=False)\n",
            ")\n"
          ]
        }
      ],
      "id": "xGxgMXXEho5n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fdc1fa2a-c124-4cab-9d79-a813ce3db0a1",
      "metadata": {
        "id": "fdc1fa2a-c124-4cab-9d79-a813ce3db0a1"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(GoogleBinaryCNN.parameters(), lr=0.005)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs = 10\n",
        "\n",
        "# initialize tracker for minimum validation loss\n",
        "valid_loss_min = np.Inf  # set initial \"min\" to infinity\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    train_loss = 0.0\n",
        "    valid_loss = 0.0\n",
        "\n",
        "    GoogleBinaryCNN.train()\n",
        "    for data, target in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        data = data.float()\n",
        "        data = data.to(device)\n",
        "        output = GoogleBinaryCNN(data)\n",
        "        target = target.long()\n",
        "        target = target.to(device)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item() * data.size(0)\n",
        "\n",
        "    GoogleBinaryCNN.eval()\n",
        "    for data, target in valid_loader:\n",
        "        data = data.float()\n",
        "        data = data.to(device)\n",
        "        output = GoogleBinaryCNN(data)\n",
        "        target = target.long()  # Convert target to torch.long\n",
        "        target = target.to(device)\n",
        "        loss = criterion(output, target)\n",
        "        valid_loss += loss.item() * data.size(0)\n",
        "\n",
        "    train_loss = train_loss / (len(train_loader) * batch_size)\n",
        "    valid_loss = valid_loss / (len(valid_loader) * batch_size)\n",
        "\n",
        "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
        "        epoch + 1,\n",
        "        train_loss,\n",
        "        valid_loss\n",
        "    ))\n",
        "\n",
        "    if valid_loss <= valid_loss_min:\n",
        "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
        "            valid_loss_min,\n",
        "            valid_loss))\n",
        "        torch.save(GoogleBinaryCNN.state_dict(), 'GoogleBinaryCNN.pt')\n",
        "        valid_loss_min = valid_loss\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2466a6c2-be2b-4e6e-caef-1b034e1b1375",
        "id": "EeP4aablwpjs"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 \tTraining Loss: 0.692319 \tValidation Loss: 0.690934\n",
            "Validation loss decreased (inf --> 0.690934).  Saving model ...\n",
            "Epoch: 2 \tTraining Loss: 0.688618 \tValidation Loss: 0.686410\n",
            "Validation loss decreased (0.690934 --> 0.686410).  Saving model ...\n",
            "Epoch: 3 \tTraining Loss: 0.684175 \tValidation Loss: 0.682329\n",
            "Validation loss decreased (0.686410 --> 0.682329).  Saving model ...\n",
            "Epoch: 4 \tTraining Loss: 0.680013 \tValidation Loss: 0.677336\n",
            "Validation loss decreased (0.682329 --> 0.677336).  Saving model ...\n",
            "Epoch: 5 \tTraining Loss: 0.672881 \tValidation Loss: 0.666637\n",
            "Validation loss decreased (0.677336 --> 0.666637).  Saving model ...\n",
            "Epoch: 6 \tTraining Loss: 0.654480 \tValidation Loss: 0.623516\n",
            "Validation loss decreased (0.666637 --> 0.623516).  Saving model ...\n",
            "Epoch: 7 \tTraining Loss: 0.570193 \tValidation Loss: 0.515572\n",
            "Validation loss decreased (0.623516 --> 0.515572).  Saving model ...\n",
            "Epoch: 8 \tTraining Loss: 0.492142 \tValidation Loss: 0.460515\n",
            "Validation loss decreased (0.515572 --> 0.460515).  Saving model ...\n",
            "Epoch: 9 \tTraining Loss: 0.454267 \tValidation Loss: 0.433468\n",
            "Validation loss decreased (0.460515 --> 0.433468).  Saving model ...\n",
            "Epoch: 10 \tTraining Loss: 0.429350 \tValidation Loss: 0.412826\n",
            "Validation loss decreased (0.433468 --> 0.412826).  Saving model ...\n"
          ]
        }
      ],
      "id": "EeP4aablwpjs"
    },
    {
      "cell_type": "code",
      "source": [
        "GoogleBinaryCNN.load_state_dict(torch.load('GoogleBinaryCNN.pt'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NHDZEcbgpXAF",
        "outputId": "56fbf521-7d85-4d26-e8b3-3f8ec5c17966"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "id": "NHDZEcbgpXAF"
    },
    {
      "cell_type": "code",
      "source": [
        "test_loader = torch.utils.data.DataLoader(test_data_cnn_google_binary, batch_size=1)\n"
      ],
      "metadata": {
        "id": "ZQmz8TF2panP"
      },
      "execution_count": null,
      "outputs": [],
      "id": "ZQmz8TF2panP"
    },
    {
      "cell_type": "code",
      "source": [
        "print('Accuracy of CNN using  Google Word2Vec vectors (Binary) :',str(predict(GoogleBinaryCNN, test_loader)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da8a4f05-d05f-4b55-f73b-bd7fb2f7fcff",
        "id": "WEfyR6iwwpjs"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of CNN using  Google Word2Vec vectors (Binary) : 0.81615\n"
          ]
        }
      ],
      "id": "WEfyR6iwwpjs"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CNN binary custom model"
      ],
      "metadata": {
        "id": "oeA4TZtsOlFQ"
      },
      "id": "oeA4TZtsOlFQ"
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_cnn_custom_binary = TrainReviewCNN(X_train_raw_binary, Y_train_raw_binary, wv_custom, \"custom\")\n",
        "test_data_cnn_custom_binary = TestReviewCNN(X_test_raw_binary, Y_test_raw_binary, wv_custom, \"custom\")"
      ],
      "metadata": {
        "id": "oKM7zqaXzmXu"
      },
      "execution_count": null,
      "outputs": [],
      "id": "oKM7zqaXzmXu"
    },
    {
      "cell_type": "code",
      "source": [
        "# how many samples per batch to load\n",
        "batch_size = 100\n",
        "# percentage of training set to use as validation\n",
        "valid_size = 0.2\n",
        "\n",
        "# obtain training indices that will be used for validation\n",
        "num_train = len(train_data_cnn_custom_binary)\n",
        "indices = list(range(num_train))\n",
        "np.random.shuffle(indices)\n",
        "split = int(np.floor(valid_size * num_train))\n",
        "train_idx, valid_idx = indices[split:], indices[:split]\n",
        "\n",
        "# define samplers for obtaining training and validation batches\n",
        "train_sampler = SubsetRandomSampler(train_idx)\n",
        "valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "\n",
        "# prepare data loaders\n",
        "train_loader = torch.utils.data.DataLoader(train_data_cnn_custom_binary, batch_size=batch_size, sampler=train_sampler)\n",
        "valid_loader = torch.utils.data.DataLoader(train_data_cnn_custom_binary, batch_size=batch_size, sampler=valid_sampler)\n",
        "test_loader = torch.utils.data.DataLoader(test_data_cnn_custom_binary, batch_size=batch_size)\n"
      ],
      "metadata": {
        "id": "toJD8Mbhzw3r"
      },
      "execution_count": null,
      "outputs": [],
      "id": "toJD8Mbhzw3r"
    },
    {
      "cell_type": "code",
      "source": [
        "CustomBinaryCNN = BinaryCNN().to(device)\n",
        "print(CustomBinaryCNN)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85655cd1-b0e2-4a32-e88f-1ef9cb1ae4ca",
        "id": "6aJ-oaKwxESx"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BinaryCNN(\n",
            "  (conv1): Conv1d(300, 50, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "  (conv2): Conv1d(50, 10, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "  (pool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (fc1): Linear(in_features=120, out_features=2, bias=True)\n",
            "  (dropout1): Dropout(p=0.3, inplace=False)\n",
            "  (dropout2): Dropout(p=0.3, inplace=False)\n",
            ")\n"
          ]
        }
      ],
      "id": "6aJ-oaKwxESx"
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(CustomBinaryCNN.parameters(), lr=0.005)\n"
      ],
      "metadata": {
        "id": "mVbkrwSMz9Vr"
      },
      "execution_count": null,
      "outputs": [],
      "id": "mVbkrwSMz9Vr"
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs = 10\n",
        "\n",
        "# initialize tracker for minimum validation loss\n",
        "valid_loss_min = np.Inf  # set initial \"min\" to infinity\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    train_loss = 0.0\n",
        "    valid_loss = 0.0\n",
        "\n",
        "    CustomBinaryCNN.train()\n",
        "    for data, target in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        data = data.float()\n",
        "        data = data.to(device)\n",
        "        output = CustomBinaryCNN(data)\n",
        "        target = target.long()\n",
        "        target = target.to(device)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item() * data.size(0)\n",
        "\n",
        "    CustomBinaryCNN.eval()\n",
        "    for data, target in valid_loader:\n",
        "        data = data.float()\n",
        "        data = data.to(device)\n",
        "        output = CustomBinaryCNN(data)\n",
        "        target = target.long()  # Convert target to torch.long\n",
        "        target = target.to(device)\n",
        "        loss = criterion(output, target)\n",
        "        valid_loss += loss.item() * data.size(0)\n",
        "\n",
        "    train_loss = train_loss / (len(train_loader) * batch_size)\n",
        "    valid_loss = valid_loss / (len(valid_loader) * batch_size)\n",
        "\n",
        "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
        "        epoch + 1,\n",
        "        train_loss,\n",
        "        valid_loss\n",
        "    ))\n",
        "\n",
        "    if valid_loss <= valid_loss_min:\n",
        "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
        "            valid_loss_min,\n",
        "            valid_loss))\n",
        "        torch.save(CustomBinaryCNN.state_dict(), 'CustomBinaryCNN.pt')\n",
        "        valid_loss_min = valid_loss\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3987af63-2ca1-47de-9e24-119a9afd9ad2",
        "id": "iURce9_IxSdK"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 \tTraining Loss: 0.688606 \tValidation Loss: 0.663523\n",
            "Validation loss decreased (inf --> 0.663523).  Saving model ...\n",
            "Epoch: 2 \tTraining Loss: 0.526122 \tValidation Loss: 0.418610\n",
            "Validation loss decreased (0.663523 --> 0.418610).  Saving model ...\n",
            "Epoch: 3 \tTraining Loss: 0.394990 \tValidation Loss: 0.364230\n",
            "Validation loss decreased (0.418610 --> 0.364230).  Saving model ...\n",
            "Epoch: 4 \tTraining Loss: 0.358007 \tValidation Loss: 0.337522\n",
            "Validation loss decreased (0.364230 --> 0.337522).  Saving model ...\n",
            "Epoch: 5 \tTraining Loss: 0.336285 \tValidation Loss: 0.323360\n",
            "Validation loss decreased (0.337522 --> 0.323360).  Saving model ...\n",
            "Epoch: 6 \tTraining Loss: 0.322404 \tValidation Loss: 0.312290\n",
            "Validation loss decreased (0.323360 --> 0.312290).  Saving model ...\n",
            "Epoch: 7 \tTraining Loss: 0.312077 \tValidation Loss: 0.304687\n",
            "Validation loss decreased (0.312290 --> 0.304687).  Saving model ...\n",
            "Epoch: 8 \tTraining Loss: 0.305302 \tValidation Loss: 0.299005\n",
            "Validation loss decreased (0.304687 --> 0.299005).  Saving model ...\n",
            "Epoch: 9 \tTraining Loss: 0.298835 \tValidation Loss: 0.298837\n",
            "Validation loss decreased (0.299005 --> 0.298837).  Saving model ...\n",
            "Epoch: 10 \tTraining Loss: 0.293248 \tValidation Loss: 0.290513\n",
            "Validation loss decreased (0.298837 --> 0.290513).  Saving model ...\n"
          ]
        }
      ],
      "id": "iURce9_IxSdK"
    },
    {
      "cell_type": "code",
      "source": [
        "CustomBinaryCNN.load_state_dict(torch.load('CustomBinaryCNN.pt'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQe8S9K40N23",
        "outputId": "480b7f02-3bbf-463c-ea2b-dde5c71a4c79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "id": "uQe8S9K40N23"
    },
    {
      "cell_type": "code",
      "source": [
        "test_loader = torch.utils.data.DataLoader(test_data_cnn_custom_binary, batch_size=1)\n"
      ],
      "metadata": {
        "id": "3UH8lZQc8zl0"
      },
      "execution_count": null,
      "outputs": [],
      "id": "3UH8lZQc8zl0"
    },
    {
      "cell_type": "code",
      "source": [
        "print('Accuracy of CNN using  Custom Word2Vec vectors (Binary) :',str(predict(CustomBinaryCNN, test_loader)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2316df5b-25ef-4fe3-c9b4-5ea4ba4bd3d1",
        "id": "XVoWm33ExYvn"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of CNN using  Custom Word2Vec vectors (Binary) : 0.878325\n"
          ]
        }
      ],
      "id": "XVoWm33ExYvn"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CNN Ternary Google model"
      ],
      "metadata": {
        "id": "CeVIQtXHOyXO"
      },
      "id": "CeVIQtXHOyXO"
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_raw_ternary, X_test_raw_ternary, Y_train_raw_ternary, Y_test_raw_ternary = train_test_split(downsized_df['review_body'], downsized_df['sentiment'], test_size=0.2, random_state=48)\n"
      ],
      "metadata": {
        "id": "a13-bvKfppam"
      },
      "execution_count": null,
      "outputs": [],
      "id": "a13-bvKfppam"
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_cnn_google_ternary = TrainReviewCNN(X_train_raw_ternary, Y_train_raw_ternary, wv, \"google\")\n",
        "test_data_cnn_google_ternary = TestReviewCNN(X_test_raw_ternary, Y_test_raw_ternary, wv, \"google\")"
      ],
      "metadata": {
        "id": "CIrsrtBjC1ID"
      },
      "execution_count": null,
      "outputs": [],
      "id": "CIrsrtBjC1ID"
    },
    {
      "cell_type": "code",
      "source": [
        "# how many samples per batch to load\n",
        "batch_size = 100\n",
        "# percentage of training set to use as validation\n",
        "valid_size = 0.2\n",
        "\n",
        "# obtain training indices that will be used for validation\n",
        "num_train = len(train_data_cnn_google_ternary)\n",
        "indices = list(range(num_train))\n",
        "np.random.shuffle(indices)\n",
        "split = int(np.floor(valid_size * num_train))\n",
        "train_idx, valid_idx = indices[split:], indices[:split]\n",
        "\n",
        "# define samplers for obtaining training and validation batches\n",
        "train_sampler = SubsetRandomSampler(train_idx)\n",
        "valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "\n",
        "# prepare data loaders\n",
        "train_loader = torch.utils.data.DataLoader(train_data_cnn_google_ternary, batch_size=batch_size, sampler=train_sampler)\n",
        "valid_loader = torch.utils.data.DataLoader(train_data_cnn_google_ternary, batch_size=batch_size, sampler=valid_sampler)\n",
        "test_loader = torch.utils.data.DataLoader(test_data_cnn_google_ternary, batch_size=batch_size)\n"
      ],
      "metadata": {
        "id": "8QnjJp6tC8gQ"
      },
      "execution_count": null,
      "outputs": [],
      "id": "8QnjJp6tC8gQ"
    },
    {
      "cell_type": "code",
      "source": [
        "class TernaryCNN(nn.Module):\n",
        "    def __init__(self, output_channels1=50, output_channels2=10, max_length=50, vector_size=300):\n",
        "        super(TernaryCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(in_channels=vector_size, out_channels=output_channels1, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv1d(in_channels=output_channels1, out_channels=output_channels2, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool1d(2)\n",
        "\n",
        "        self.fc1 = nn.Linear(120, 3)\n",
        "\n",
        "        self.dropout1 = nn.Dropout(0.3)\n",
        "        self.dropout2 = nn.Dropout(0.3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.pool(self.conv1(x)))\n",
        "        x = self.dropout1(x)\n",
        "        x = F.relu(self.pool(self.conv2(x)))\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        return x\n",
        "\n"
      ],
      "metadata": {
        "id": "vJlS6AeCDA1X"
      },
      "execution_count": null,
      "outputs": [],
      "id": "vJlS6AeCDA1X"
    },
    {
      "cell_type": "code",
      "source": [
        "GoogleTernaryModel = TernaryCNN().to(device)\n",
        "print(GoogleTernaryModel)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94d6f005-51b9-454a-a773-c818e210970e",
        "id": "5MiGDoRrxk9s"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TernaryCNN(\n",
            "  (conv1): Conv1d(300, 50, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "  (conv2): Conv1d(50, 10, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "  (pool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (fc1): Linear(in_features=120, out_features=3, bias=True)\n",
            "  (dropout1): Dropout(p=0.3, inplace=False)\n",
            "  (dropout2): Dropout(p=0.3, inplace=False)\n",
            ")\n"
          ]
        }
      ],
      "id": "5MiGDoRrxk9s"
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(GoogleTernaryModel.parameters(), lr=0.005)\n"
      ],
      "metadata": {
        "id": "p7YQz7JjDJfO"
      },
      "execution_count": null,
      "outputs": [],
      "id": "p7YQz7JjDJfO"
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs = 15\n",
        "\n",
        "# initialize tracker for minimum validation loss\n",
        "valid_loss_min = np.Inf  # set initial \"min\" to infinity\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    train_loss = 0.0\n",
        "    valid_loss = 0.0\n",
        "\n",
        "    GoogleTernaryModel.train()\n",
        "    for data, target in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        data = data.float()\n",
        "        data = data.to(device)\n",
        "        output = GoogleTernaryModel(data)\n",
        "        target = target.long()\n",
        "        target = target.to(device)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item() * data.size(0)\n",
        "\n",
        "    GoogleTernaryModel.eval()\n",
        "    for data, target in valid_loader:\n",
        "        data = data.float()\n",
        "        data = data.to(device)\n",
        "        output = GoogleTernaryModel(data)\n",
        "        target = target.long()  # Convert target to torch.long\n",
        "        target = target.to(device)\n",
        "        loss = criterion(output, target)\n",
        "        valid_loss += loss.item() * data.size(0)\n",
        "\n",
        "    train_loss = train_loss / (len(train_loader) * batch_size)\n",
        "    valid_loss = valid_loss / (len(valid_loader) * batch_size)\n",
        "\n",
        "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
        "        epoch + 1,\n",
        "        train_loss,\n",
        "        valid_loss\n",
        "    ))\n",
        "\n",
        "    if valid_loss <= valid_loss_min:\n",
        "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
        "            valid_loss_min,\n",
        "            valid_loss))\n",
        "        torch.save(GoogleTernaryModel.state_dict(), 'GoogleTernaryModel.pt')\n",
        "        valid_loss_min = valid_loss\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79f22fca-77de-4f31-9fea-b15061569ae1",
        "id": "8l1ZQ9UwxvVY"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 \tTraining Loss: 1.091424 \tValidation Loss: 1.086575\n",
            "Validation loss decreased (inf --> 1.086575).  Saving model ...\n",
            "Epoch: 2 \tTraining Loss: 1.083366 \tValidation Loss: 1.080107\n",
            "Validation loss decreased (1.086575 --> 1.080107).  Saving model ...\n",
            "Epoch: 3 \tTraining Loss: 1.063947 \tValidation Loss: 1.037872\n",
            "Validation loss decreased (1.080107 --> 1.037872).  Saving model ...\n",
            "Epoch: 4 \tTraining Loss: 1.022200 \tValidation Loss: 1.001029\n",
            "Validation loss decreased (1.037872 --> 1.001029).  Saving model ...\n",
            "Epoch: 5 \tTraining Loss: 0.972040 \tValidation Loss: 0.936014\n",
            "Validation loss decreased (1.001029 --> 0.936014).  Saving model ...\n",
            "Epoch: 6 \tTraining Loss: 0.906739 \tValidation Loss: 0.875824\n",
            "Validation loss decreased (0.936014 --> 0.875824).  Saving model ...\n",
            "Epoch: 7 \tTraining Loss: 0.860525 \tValidation Loss: 0.837321\n",
            "Validation loss decreased (0.875824 --> 0.837321).  Saving model ...\n",
            "Epoch: 8 \tTraining Loss: 0.832828 \tValidation Loss: 0.816177\n",
            "Validation loss decreased (0.837321 --> 0.816177).  Saving model ...\n",
            "Epoch: 9 \tTraining Loss: 0.812674 \tValidation Loss: 0.801587\n",
            "Validation loss decreased (0.816177 --> 0.801587).  Saving model ...\n",
            "Epoch: 10 \tTraining Loss: 0.796512 \tValidation Loss: 0.784421\n",
            "Validation loss decreased (0.801587 --> 0.784421).  Saving model ...\n",
            "Epoch: 11 \tTraining Loss: 0.783880 \tValidation Loss: 0.773475\n",
            "Validation loss decreased (0.784421 --> 0.773475).  Saving model ...\n",
            "Epoch: 12 \tTraining Loss: 0.772610 \tValidation Loss: 0.763185\n",
            "Validation loss decreased (0.773475 --> 0.763185).  Saving model ...\n",
            "Epoch: 13 \tTraining Loss: 0.763771 \tValidation Loss: 0.754811\n",
            "Validation loss decreased (0.763185 --> 0.754811).  Saving model ...\n",
            "Epoch: 14 \tTraining Loss: 0.755533 \tValidation Loss: 0.750332\n",
            "Validation loss decreased (0.754811 --> 0.750332).  Saving model ...\n",
            "Epoch: 15 \tTraining Loss: 0.748132 \tValidation Loss: 0.742058\n",
            "Validation loss decreased (0.750332 --> 0.742058).  Saving model ...\n"
          ]
        }
      ],
      "id": "8l1ZQ9UwxvVY"
    },
    {
      "cell_type": "code",
      "source": [
        "GoogleTernaryModel.load_state_dict(torch.load('GoogleTernaryModel.pt'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8IvdaZkqDcDN",
        "outputId": "6193a2c1-ef23-430c-f39d-f27c4d67e5bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "id": "8IvdaZkqDcDN"
    },
    {
      "cell_type": "code",
      "source": [
        "test_loader = torch.utils.data.DataLoader(test_data_cnn_google_ternary, batch_size=1)\n"
      ],
      "metadata": {
        "id": "_B82Fa5sDkd2"
      },
      "execution_count": null,
      "outputs": [],
      "id": "_B82Fa5sDkd2"
    },
    {
      "cell_type": "code",
      "source": [
        "print('Accuracy of CNN using  Google Word2Vec vectors (Ternary) :',str(predict(GoogleTernaryModel, test_loader)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c641092d-930b-499e-dec8-bb34ac0be9cd",
        "id": "A8RgZJp_xvVY"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of CNN using  Google Word2Vec vectors (Ternary) : 0.6844\n"
          ]
        }
      ],
      "id": "A8RgZJp_xvVY"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CNN Ternary Custom Model"
      ],
      "metadata": {
        "id": "PLnTfc_WPHmG"
      },
      "id": "PLnTfc_WPHmG"
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_cnn_custom_ternary = TrainReviewCNN(X_train_raw_ternary, Y_train_raw_ternary, wv_custom, \"custom\")\n",
        "test_data_cnn_custom_ternary = TestReviewCNN(X_test_raw_ternary, Y_test_raw_ternary, wv_custom, \"custom\")"
      ],
      "metadata": {
        "id": "pxj6HThQ-Ra6"
      },
      "execution_count": null,
      "outputs": [],
      "id": "pxj6HThQ-Ra6"
    },
    {
      "cell_type": "code",
      "source": [
        "# how many samples per batch to load\n",
        "batch_size = 100\n",
        "# percentage of training set to use as validation\n",
        "valid_size = 0.2\n",
        "\n",
        "# obtain training indices that will be used for validation\n",
        "num_train = len(train_data_cnn_custom_ternary)\n",
        "indices = list(range(num_train))\n",
        "np.random.shuffle(indices)\n",
        "split = int(np.floor(valid_size * num_train))\n",
        "train_idx, valid_idx = indices[split:], indices[:split]\n",
        "\n",
        "# define samplers for obtaining training and validation batches\n",
        "train_sampler = SubsetRandomSampler(train_idx)\n",
        "valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "\n",
        "# prepare data loaders\n",
        "train_loader = torch.utils.data.DataLoader(train_data_cnn_custom_ternary, batch_size=batch_size, sampler=train_sampler)\n",
        "valid_loader = torch.utils.data.DataLoader(train_data_cnn_custom_ternary, batch_size=batch_size, sampler=valid_sampler)\n",
        "test_loader = torch.utils.data.DataLoader(test_data_cnn_custom_ternary, batch_size=batch_size)\n"
      ],
      "metadata": {
        "id": "n7lLIqN_-pWy"
      },
      "execution_count": null,
      "outputs": [],
      "id": "n7lLIqN_-pWy"
    },
    {
      "cell_type": "code",
      "source": [
        "CustomTernaryModel = TernaryCNN().to(device)\n",
        "print(CustomTernaryModel)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g3QsSGNf-6Hm",
        "outputId": "0bddcf43-775b-4fb0-d0bb-6b27dc95c69b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TernaryCNN(\n",
            "  (conv1): Conv1d(300, 50, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "  (conv2): Conv1d(50, 10, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "  (pool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (fc1): Linear(in_features=120, out_features=3, bias=True)\n",
            "  (dropout1): Dropout(p=0.3, inplace=False)\n",
            "  (dropout2): Dropout(p=0.3, inplace=False)\n",
            ")\n"
          ]
        }
      ],
      "id": "g3QsSGNf-6Hm"
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(CustomTernaryModel.parameters(), lr=0.005)\n"
      ],
      "metadata": {
        "id": "-HO-dLlq_JnY"
      },
      "execution_count": null,
      "outputs": [],
      "id": "-HO-dLlq_JnY"
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs = 15\n",
        "\n",
        "# initialize tracker for minimum validation loss\n",
        "valid_loss_min = np.Inf  # set initial \"min\" to infinity\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    train_loss = 0.0\n",
        "    valid_loss = 0.0\n",
        "\n",
        "    CustomTernaryModel.train()\n",
        "    for data, target in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        data = data.float()\n",
        "        data = data.to(device)\n",
        "        output = CustomTernaryModel(data)\n",
        "        target = target.long()\n",
        "        target = target.to(device)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item() * data.size(0)\n",
        "\n",
        "    CustomTernaryModel.eval()\n",
        "    for data, target in valid_loader:\n",
        "        data = data.float()\n",
        "        data = data.to(device)\n",
        "        output = CustomTernaryModel(data)\n",
        "        target = target.long()  # Convert target to torch.long\n",
        "        target = target.to(device)\n",
        "        loss = criterion(output, target)\n",
        "        valid_loss += loss.item() * data.size(0)\n",
        "\n",
        "    train_loss = train_loss / (len(train_loader) * batch_size)\n",
        "    valid_loss = valid_loss / (len(valid_loader) * batch_size)\n",
        "\n",
        "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
        "        epoch + 1,\n",
        "        train_loss,\n",
        "        valid_loss\n",
        "    ))\n",
        "\n",
        "    if valid_loss <= valid_loss_min:\n",
        "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
        "            valid_loss_min,\n",
        "            valid_loss))\n",
        "        torch.save(CustomTernaryModel.state_dict(), 'CustomTernaryModel.pt')\n",
        "        valid_loss_min = valid_loss\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4jgyoj4_Xdq",
        "outputId": "8be4b0da-24ff-4922-c68b-e37f1711037c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 \tTraining Loss: 0.990258 \tValidation Loss: 0.846398\n",
            "Validation loss decreased (inf --> 0.846398).  Saving model ...\n",
            "Epoch: 2 \tTraining Loss: 0.810289 \tValidation Loss: 0.759780\n",
            "Validation loss decreased (0.846398 --> 0.759780).  Saving model ...\n",
            "Epoch: 3 \tTraining Loss: 0.751358 \tValidation Loss: 0.719146\n",
            "Validation loss decreased (0.759780 --> 0.719146).  Saving model ...\n",
            "Epoch: 4 \tTraining Loss: 0.721840 \tValidation Loss: 0.700711\n",
            "Validation loss decreased (0.719146 --> 0.700711).  Saving model ...\n",
            "Epoch: 5 \tTraining Loss: 0.706094 \tValidation Loss: 0.688481\n",
            "Validation loss decreased (0.700711 --> 0.688481).  Saving model ...\n",
            "Epoch: 6 \tTraining Loss: 0.694702 \tValidation Loss: 0.679873\n",
            "Validation loss decreased (0.688481 --> 0.679873).  Saving model ...\n",
            "Epoch: 7 \tTraining Loss: 0.685925 \tValidation Loss: 0.675176\n",
            "Validation loss decreased (0.679873 --> 0.675176).  Saving model ...\n",
            "Epoch: 8 \tTraining Loss: 0.678519 \tValidation Loss: 0.667899\n",
            "Validation loss decreased (0.675176 --> 0.667899).  Saving model ...\n",
            "Epoch: 9 \tTraining Loss: 0.672188 \tValidation Loss: 0.662677\n",
            "Validation loss decreased (0.667899 --> 0.662677).  Saving model ...\n",
            "Epoch: 10 \tTraining Loss: 0.665855 \tValidation Loss: 0.660789\n",
            "Validation loss decreased (0.662677 --> 0.660789).  Saving model ...\n",
            "Epoch: 11 \tTraining Loss: 0.661823 \tValidation Loss: 0.656491\n",
            "Validation loss decreased (0.660789 --> 0.656491).  Saving model ...\n",
            "Epoch: 12 \tTraining Loss: 0.656920 \tValidation Loss: 0.652847\n",
            "Validation loss decreased (0.656491 --> 0.652847).  Saving model ...\n",
            "Epoch: 13 \tTraining Loss: 0.653420 \tValidation Loss: 0.651322\n",
            "Validation loss decreased (0.652847 --> 0.651322).  Saving model ...\n",
            "Epoch: 14 \tTraining Loss: 0.649856 \tValidation Loss: 0.648811\n",
            "Validation loss decreased (0.651322 --> 0.648811).  Saving model ...\n",
            "Epoch: 15 \tTraining Loss: 0.645916 \tValidation Loss: 0.646246\n",
            "Validation loss decreased (0.648811 --> 0.646246).  Saving model ...\n"
          ]
        }
      ],
      "id": "b4jgyoj4_Xdq"
    },
    {
      "cell_type": "code",
      "source": [
        "CustomTernaryModel.load_state_dict(torch.load('CustomTernaryModel.pt'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j8QbclSd_eTT",
        "outputId": "a19da937-f049-457f-c34a-5b052e473559"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "id": "j8QbclSd_eTT"
    },
    {
      "cell_type": "code",
      "source": [
        "test_loader = torch.utils.data.DataLoader(test_data_cnn_custom_ternary, batch_size=1)\n"
      ],
      "metadata": {
        "id": "qxx3dvm0_rIT"
      },
      "execution_count": null,
      "outputs": [],
      "id": "qxx3dvm0_rIT"
    },
    {
      "cell_type": "code",
      "source": [
        "print('Accuracy of CNN using  Custom Word2Vec vectors (Ternary) :',str(predict(CustomTernaryModel, test_loader)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bYNbhoco_zpF",
        "outputId": "bba71680-6c2f-439e-f76c-0dc96b837cfc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of CNN using  Custom Word2Vec vectors (Ternary) : 0.7288\n"
          ]
        }
      ],
      "id": "bYNbhoco_zpF"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}